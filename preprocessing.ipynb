{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aedb3c9",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "é€™è£¡æœƒå­˜æ”¾æ‰€æœ‰è³‡æ–™å‰è™•ç†åŠè³‡æ–™åˆ†æçš„function\n",
    "\n",
    "\n",
    "### æª”æ¡ˆèªªæ˜\n",
    "æœƒä½¿ç”¨åˆ°raw dataï¼Œç„¶å¾Œé€™è£¡åšåˆ†æå–å¾—æŒ‡æ¨™ä¹‹å¾Œæœƒå¯«å›raw data(æœƒæ›´æ–°åŸæª”)ï¼Œæ‰€ä»¥æœ‰éœ€è¦ä¿ç•™åŸæª”æ¡ˆçš„è©±è«‹è‡ªè¡Œå‚™ä»½rawdata(æˆ–ä½¿ç”¨å‡½æ•¸copy_rawData)\n",
    "\n",
    "### ç¨‹å¼é‚è¼¯\n",
    "- æ¯ä¸€å€‹functionéƒ½åªåšä¸€ä»¶äº‹(ä¾‹å¦‚æˆ‘è¦è¨ˆç®—åƒèˆ‡åº¦ä¸¦ä¸”æŠŠåƒèˆ‡åº¦å¯«å›csvï¼Œæˆ‘æœƒç”¨å…©å€‹å‡½æ•¸å»åšé€™ä»¶äº‹ï¼ŒAå‡½æ•¸ç®—åƒèˆ‡åº¦ï¼ŒBå‡½æ•¸å¯«æª”æ¡ˆ)\n",
    "- æ¯å€‹å‡½æ•¸çš„åƒæ•¸éƒ½å·²ç¶“æ¨™è¨»è³‡æ–™å‹æ…‹ï¼Œè³‡æ–™å‹æ…‹ä¸å°çš„è©±è·‘ä¸å‹•é€™å€‹éœ€è¦æ³¨æ„ä¸€ä¸‹\n",
    "- éƒ¨åˆ†é‡è¤‡åŸ·è¡Œçš„å‡½æ•¸æˆ‘æŠŠæ‹¿ä¾†ç•¶ä½œlogçš„printè¨»è§£æ‰äº†(ä¸ç„¶æœƒout of memory)\n",
    "- funtionä¸‹é¢æœƒæœ‰ç¯„ä¾‹çš„è¼¸å…¥ï¼Œå¯ä»¥æ‹¿ä¾†æ¸¬è©¦çœ‹çœ‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93148d1b",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc869db",
   "metadata": {},
   "source": [
    "create_folder: å‚™ä»½æª”æ¡ˆç”¨ï¼Œæª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°±å‰µå»º  \n",
    "copy_rawData: å‚™ä»½æª”æ¡ˆç”¨ï¼Œæœƒè¤‡è£½å–®ä¸€csvæª”æ¡ˆåˆ°å¦ä¸€å€‹ç›®éŒ„ä¹‹ä¸­  \n",
    "create_txt_file: é–‹å°ç­†è¨˜ç´€éŒ„ä¸€äº›æŒ‡æ¨™ç”¨çš„ã€‚è‹¥æª”æ¡ˆå­˜åœ¨å°±é–‹æª”ç›´æ¥ç¹¼çºŒæ–°å¢è³‡æ–™åˆ°ä¸‹ä¸€è¡Œï¼Œä¸å­˜åœ¨å‰‡å‰µå»ºæª”æ¡ˆ  \n",
    "read_csv_files_in_directory: è®€å–æ”¾åœ¨ç›®éŒ„ä¹‹ä¸‹çš„æ‰€æœ‰csvæª”æ¡ˆ(æ‰€ä»¥æˆ‘æœƒæŠŠåŒé¡å‹çš„æª”æ¡ˆæ•´ç†åˆ°åŒä¸€å€‹ç›®éŒ„ä¸‹åšæ‰¹æ¬¡è™•ç†)  \n",
    "get_Col_data: æŠŠéœ€è¦ç”¨åˆ°çš„æ•¸å€‹ç‰¹å¾µè³‡æ–™åˆ—çš„è³‡æ–™æ’ˆå‡ºä¾†(å› ç‚ºåˆ†ææ™‚é€šå¸¸æœƒéœ€è¦å¾ˆå¤šç‰¹å¾µ)  \n",
    "Add_Collumn_to_file: æŠŠè¨ˆç®—å¾—å‡ºçš„çµæœåŠ å›å‰µä½œè€…çš„è³‡æ–™æª”ä¹‹ä¸­(æœ€å¾Œé æ¸¬æœƒç”¨é€™å€‹æª”æ¡ˆ)  \n",
    "count_creator_engagement: è¨ˆç®—å‰µä½œè€…åƒèˆ‡åº¦(å…¬å¼=åƒèˆ‡/ç²‰çµ²æ•¸)  \n",
    "count_engagement_Threshold: creator_engagementå–å¹³å‡å¾—åˆ°engagement_Threshold  \n",
    "judje_creator_success: æ ¹æ“šcreator_engagementèˆ‡engagement_Thresholdåˆ¤æ–·æ˜¯å¦æˆåŠŸ    \n",
    "str_To_Number: åšfollowerçš„å‰è™•ç†  \n",
    "Classify_influencerType: æ ¹æ“šç¶²ç´…å®šç¾©åˆ†é¡  \n",
    "\n",
    "\n",
    "detect_urls_hashtags_metions: æª¢æ¸¬æ­¤ä½œè€…çš„æ–‡ç« æ˜¯å¦æœ‰hashtagã€ç¶²å€ï¼Œæœ‰çš„è©±æ¨™è¨»åŒ…å«æ¬¡æ•¸åœ¨æª”æ¡ˆä¸­  \n",
    "detect_emoji   \n",
    "calculate_monthly_post_stability: è¨ˆç®—ç™¼æ–‡ç©©å®šåº¦   \n",
    "text_preprocessing: åŒ…å«ç¹ªæ–‡å­—çš„è©±ä¸€æ¨£è¨ˆç®—å‡ºç¾æ¬¡æ•¸ï¼Œä¸¦å°‡ç¹ªæ–‡å­—è½‰å›æƒ…ç·’è©å½™  \n",
    "get_wordnet_pos: å‰è™•ç†ç”¨ã€‚Function to map Penn Treebank POS tag to WordNet POS tag  \n",
    "clean_text: å‰è™•ç†ç”¨ã€‚ç§»é™¤ç„¡æ³•åˆ†æçš„å­—å…ƒä¸¦åšæ–·è©  \n",
    "Sentiment_Analysis: è¨ˆç®—æ¯ä¸€ç¯‡è²¼æ–‡çš„æƒ…ç·’  \n",
    "count_rate_of_post_Sentiment: è¨ˆç®—æ­£å‘æƒ…æ„Ÿè²¼æ–‡çš„æ¯”ç‡  \n",
    "count_avg_Subjectivity  \n",
    "\n",
    "\n",
    "train_LDA_model  \n",
    "topic_analysis  \n",
    "get_topic_and_write  \n",
    "symSentSim: æ¯”å°å¤§äº”äººæ ¼è¾­å…¸è¨ˆç®—OCEANäº”å¤§é …ç›®å¾—åˆ†  \n",
    "personality_analysis: å°‡åˆ†æ•¸å¯«å…¥creator_big5.csvæª”æ¡ˆä¸­  \n",
    "calculate_ocean_avg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c85c1",
   "metadata": {},
   "source": [
    "## å¼•ç”¨å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f0e3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®€å–æª”æ¡ˆ\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b480cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨Nature Language Tool Kit (NLTK)è™•ç†æ–‡æœ¬\n",
    "import nltk  \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12ebb26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('punkt')\\nnltk.download('averaged_perceptron_tagger')\\nnltk.download('vader_lexicon')\\nnltk.download('stopwords')\\nnltk.download('wordnet')\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "421db68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–‡æœ¬è™•ç†\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59a5a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "904872c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa7205",
   "metadata": {},
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca03df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°±å‰µå»ºè³‡æ–™å¤¾\n",
    "def create_folder(folder_path):\n",
    "    print('** def create_folder **')\n",
    "    try:\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"è³‡æ–™å¤¾ '{folder_path}' å‰µå»ºæˆåŠŸï¼\")\n",
    "        else:\n",
    "            print(f\"è³‡æ–™å¤¾ '{folder_path}' å·²ç¶“å­˜åœ¨ã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"å‰µå»ºè³‡æ–™å¤¾æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "# ä¿å­˜å‚™ä»½raw data\n",
    "def copy_rawData(source_file_path:str, target_file_path:str):\n",
    "    print('** def copy_rawData **')\n",
    "    # è¤‡è£½å–®ä¸€csvæª”æ¡ˆåˆ°å¦ä¸€å€‹ç›®éŒ„ä¹‹ä¸­\n",
    "    try:\n",
    "        shutil.copy(source_file_path, target_file_path)\n",
    "        #å¦ä¸€ç¨®å‚™ä»½æ–¹æ³•:\n",
    "        #df = pd.read_csv(source_file_path)\n",
    "        #df.to_csv(target_file_path, index=False)\n",
    "        print('å‚™ä»½åŸæª”æ¡ˆæˆåŠŸ')\n",
    "    except:\n",
    "        print('å‚™ä»½åŸæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤')\n",
    " \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "#new_folder = 'dataset\\userdata\\copy'\n",
    "#create_folder(new_folder)\n",
    "#copy_rawData(source_file_path='userData/Azuki_bg.csv', target_file_path= new_folder+'/Azuki_bg.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8aa0d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å–®è¨˜å€‹txtç­†è¨˜\n",
    "from datetime import datetime\n",
    "\n",
    "def create_txt_file(contents:str, txt_file_name:str):\n",
    "    print('** def create_txt_file**')\n",
    "\n",
    "    try:\n",
    "        file_path = txt_file_name + '.txt'\n",
    "        with open(file_path, 'a+') as file:\n",
    "            # ç²å–ç•¶å‰æ—¥æœŸå’Œæ™‚é–“\n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            # åœ¨æ¯æ¬¡æ–°å¢å…§å®¹æ™‚æ›è¡Œï¼Œä¸¦æ¨™è¨»å¯«æª”æ—¥æœŸå’Œæ™‚é–“\n",
    "            file.write(f\"\\n{current_time}\\n{contents}\")\n",
    "        print('æˆåŠŸå¯«å…¥æ–°è³‡è¨Š', contents, 'è‡³TXTæª”æ¡ˆ', txt_file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"å¯«TXTæª”æ¡ˆæ™‚å‡ºç¾éŒ¯èª¤: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf77686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# è¼¸å…¥æŒ‡å®šçš„ç›®éŒ„è·¯å¾‘\\ndirectory_path = \"dataset/textData\"\\nread_csv_files_in_directory(directory_path)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv_files_in_directory(directory_path:str):  \n",
    "    print('** def read_csv_files_in_directory**')\n",
    "    # æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"ç›®éŒ„ '{directory_path}' ä¸å­˜åœ¨\")\n",
    "        return\n",
    "    \n",
    "    # éæ­·ç›®éŒ„ä¸­çš„æ‰€æœ‰æª”æ¡ˆå’Œå­ç›®éŒ„\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            # ç¢ºä¿æª”æ¡ˆæ˜¯ CSV æ ¼å¼\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"æ­£åœ¨è®€å–æª”æ¡ˆ: {file_path}\")\n",
    "                \n",
    "                # ä½¿ç”¨ pandas è®€å– CSV æª”æ¡ˆ\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    # åœ¨é€™è£¡å¯ä»¥å°è³‡æ–™é€²è¡Œè™•ç†\n",
    "                   # print(df.head())  # é€™è£¡åªæ˜¯ç¤ºä¾‹ï¼Œé¡¯ç¤ºæª”æ¡ˆçš„å‰å¹¾è¡Œ\n",
    "                except Exception as e:\n",
    "                    print(f\"è®€å–æª”æ¡ˆæ™‚å‡ºç¾éŒ¯èª¤: {e}\")\n",
    "'''\n",
    "\n",
    "# è¼¸å…¥æŒ‡å®šçš„ç›®éŒ„è·¯å¾‘\n",
    "directory_path = \"dataset/textData\"\n",
    "read_csv_files_in_directory(directory_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eab0e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Col_data(df, featureCols: list)-> dict:\n",
    "    print('** def get_Col_data **')\n",
    "    feature_col_dict = {}\n",
    "    for colName in featureCols:\n",
    "        try:\n",
    "            # ä½¿ç”¨ loc å‡½æ•¸é¸å–ç‰¹å®šåˆ—çš„æ‰€æœ‰å€¼\n",
    "            feature_values = df[colName].tolist()\n",
    "            # å°‡ç‰¹å¾µå€¼æ‰“åŒ…æˆåˆ—è¡¨ä¸¦æ·»åŠ åˆ°å­—å…¸ä¸­\n",
    "            feature_col_dict.update({colName:feature_values})\n",
    "        except KeyError:\n",
    "            print(f\"æœªæ‰¾åˆ°åˆ—å {colName}\")\n",
    "    return feature_col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b53c7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAdd_Collumn_to_file(New_Collumn_Name='Col',New_Collumn_list=['test'], csv_file_path='chelice_bts.csv' )\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Add_Collumn_to_file(New_Collumn_Name:str,  New_Collumn_list:list, csv_file_path:str):\n",
    "    print('** def Add_Collumn_to_file **')\n",
    "    \n",
    "    # é–‹å•ŸèˆŠçš„ CSV æª”æ¡ˆ\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"è®€å–æª”æ¡ˆæ™‚å‡ºç¾éŒ¯èª¤: {e}\")\n",
    "        return\n",
    "    df[New_Collumn_Name] = New_Collumn_list\n",
    "\n",
    "\n",
    "\n",
    "    # æŒ‡å®šæ‚¨æƒ³è¦ä¿å­˜çš„æª”æ¡ˆè·¯å¾‘\n",
    "    output_file_path = csv_file_path\n",
    "\n",
    "# å°‡æ›´æ–°å¾Œçš„ DataFrame å¯«å…¥ CSV æª”æ¡ˆ\n",
    "    try:\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "        print(\"DataFrame å·²æˆåŠŸä¿å­˜åˆ°æª”æ¡ˆ:\", output_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"ä¿å­˜ DataFrame åˆ°æª”æ¡ˆæ™‚å‡ºç¾éŒ¯èª¤:\", e)\n",
    "    \n",
    "\n",
    "'''\n",
    "Add_Collumn_to_file(New_Collumn_Name='Col',New_Collumn_list=['test'], csv_file_path='chelice_bts.csv' )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515190d0",
   "metadata": {},
   "source": [
    "# 3.2 Creator Success Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49c10509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_creator_engagement(retweets, comments, likes, followers:int=1) -> float:\n",
    "    print('** def count_creator_engagement **')\n",
    "    \n",
    "#    retweets = strList_To_NumberList(list_with_comma=retweets)\n",
    "#    comments = strList_To_NumberList(list_with_comma=comments)\n",
    "#    likes = strList_To_NumberList(list_with_comma=likes)\n",
    "    \n",
    "    # è¨ˆç®—å‰µä½œè€…åƒèˆ‡åº¦\n",
    "    creator_engagement = (sum(retweets) + sum(comments) + sum(likes)) / followers /len(likes)\n",
    "    \n",
    "    return creator_engagement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f9f1494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nengagement_Threshold = count_engagement_Threshold('dataset/userdata')\\ncreate_txt_file(contents=str(engagement_Threshold), txt_file_name='count_engagement_Threshold')\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_engagement_Threshold(directory_path)-> float:\n",
    "    print('** def count_engagement_Threshold **')\n",
    "    \n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"ç›®éŒ„ '{directory_path}' ä¸å­˜åœ¨\")\n",
    "        return\n",
    "    \n",
    "    creator_engagement = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"æ­£åœ¨è®€å–æª”æ¡ˆ: {file_path}\")\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    creator_engagement.append(df['creator_engagement'].iloc[0]) \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"è®€å–æª”æ¡ˆæ™‚å‡ºç¾éŒ¯èª¤: {e}\")\n",
    "                    \n",
    "                    \n",
    "    creator_engagement_mean = sum(creator_engagement)/len(creator_engagement) #å–å¹³å‡\n",
    "    print('engagement_Threshold: ',creator_engagement_mean)\n",
    "    return creator_engagement_mean\n",
    "\n",
    "'''\n",
    "engagement_Threshold = count_engagement_Threshold('dataset/userdata')\n",
    "create_txt_file(contents=str(engagement_Threshold), txt_file_name='count_engagement_Threshold')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65d74295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\ndf = pd.read_csv('airasia.csv')\\nans = judje_creator_success(creator_engagement=df['creator_engagement'].iloc[0], engagement_Threshold=24415.11111111111)    \\nprint(ans)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def judje_creator_success(creator_engagement:float, engagement_Threshold: float):\n",
    "    if creator_engagement>= engagement_Threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "'''    \n",
    "df = pd.read_csv('airasia.csv')\n",
    "ans = judje_creator_success(creator_engagement=df['creator_engagement'].iloc[0], engagement_Threshold=24415.11111111111)    \n",
    "print(ans)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3088f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstr_To_Number('1,059K')\\nstr_To_Number('1,000')\\nstr_To_Number('2.9K')\\nstr_To_Number('19K')\\nstr_To_Number('3,900K')\\nstr_To_Number('3.1M')\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str_To_Number(value_with_comma:str)->int:\n",
    "    print('** def str_To_Number **')\n",
    "    print(f'value_with_comma:{value_with_comma}')\n",
    "    \n",
    "    if ',' in value_with_comma:\n",
    "        value_with_comma = value_with_comma.replace(',', '')\n",
    "    if 'K' in value_with_comma :\n",
    "        value_with_comma = value_with_comma.replace('K','000') \n",
    "    \n",
    "    if 'M' in value_with_comma:\n",
    "        value_with_comma = value_with_comma.replace('M','000000') \n",
    "\n",
    "    if '.0' in value_with_comma:\n",
    "        value_with_comma = value_with_comma.replace('.0', '')\n",
    "    if '.' in value_with_comma:\n",
    "        value_with_comma = value_with_comma.replace('.', '')\n",
    "        value_with_comma = value_with_comma.replace('0', '', 1)\n",
    "    \n",
    "    value_without_comma = value_with_comma\n",
    "    integer_value = int(value_without_comma)\n",
    "    print(f'integer_value:{integer_value}')\n",
    "    \n",
    "    return integer_value\n",
    "\n",
    "\n",
    "'''\n",
    "str_To_Number('1,059K')\n",
    "str_To_Number('1,000')\n",
    "str_To_Number('2.9K')\n",
    "str_To_Number('19K')\n",
    "str_To_Number('3,900K')\n",
    "str_To_Number('3.1M')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68046df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strList_To_NumberList(list_with_comma:list)->list:\n",
    "    print('** def strList_To_NumberList **')\n",
    "    print(f'value_with_comma:{list_with_comma}')\n",
    "    \n",
    "    list_without_comma = []\n",
    "    print('here')\n",
    "    for value_with_comma in list_with_comma:\n",
    "        if ',' in value_with_comma:\n",
    "            value_with_comma = value_with_comma.replace(',', '')\n",
    "        if 'K' in value_with_comma :\n",
    "            value_with_comma = value_with_comma.replace('K','000') \n",
    "        if 'M' in value_with_comma:\n",
    "                value_with_comma = value_with_comma.replace('M','000000') \n",
    "\n",
    "        if '.0' in value_with_comma:\n",
    "                value_with_comma = value_with_comma.replace('.0', '')\n",
    "        if '.' in value_with_comma:\n",
    "                value_with_comma = value_with_comma.replace('.', '')\n",
    "                value_with_comma = value_with_comma.replace('0', '', 1)\n",
    "        \n",
    "        value_without_comma = value_with_comma    \n",
    "        integer_value = int(value_without_comma)\n",
    "        list_without_comma.append(integer_value)\n",
    "    \n",
    "    \n",
    "    print(f'integer_value:{list_without_comma}')\n",
    "    \n",
    "    return list_without_comma\n",
    "\n",
    "\n",
    "#strList_To_NumberList(list_with_comma:list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ec97778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nClassify_influencerType('1,059K')\\nClassify_influencerType('1,000')\\nClassify_influencerType('2.9K')\\nClassify_influencerType('19K')\\nClassify_influencerType('3,900K')\\nClassify_influencerType('3.1M')\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Classify_influencerType(follower:str)-> str:\n",
    "    print('** def Classify_influencerType **')\n",
    "    \n",
    "    follower = str_To_Number(follower)\n",
    "    influencerType = ''\n",
    "    \n",
    "    if follower >= 1000000:\n",
    "        influencerType = 'MegaInfluencer'\n",
    "    elif follower < 1000000 and follower>=100000:\n",
    "        influencerType = 'MacroInfluencer'\n",
    "    elif follower < 100000 and follower>=1000:\n",
    "        influencerType = 'MicroInfluencer'\n",
    "    else:\n",
    "        influencerType = 'NanoInfluencer'\n",
    "    \n",
    "    \n",
    "    print(f'follower:{follower}, influencerType:{influencerType}')\n",
    "    \n",
    "    return influencerType \n",
    "    \n",
    "'''\n",
    "Classify_influencerType('1,059K')\n",
    "Classify_influencerType('1,000')\n",
    "Classify_influencerType('2.9K')\n",
    "Classify_influencerType('19K')\n",
    "Classify_influencerType('3,900K')\n",
    "Classify_influencerType('3.1M')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c44da1",
   "metadata": {},
   "source": [
    "## 3.3.1 Short Text Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbbcca",
   "metadata": {},
   "source": [
    "### æª¢æ¸¬æ˜¯å¦åŒ…å«hashtagã€ç¶²å€\n",
    "- æª¢æ¸¬æ­¤ä½œè€…çš„æ–‡ç« æ˜¯å¦æœ‰hashtagã€ç¶²å€ï¼Œæœ‰çš„è©±æ¨™è¨»åŒ…å«æ¬¡æ•¸åœ¨æª”æ¡ˆä¸­\n",
    "- ç§»é™¤ç¶²å€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d4aeae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# instance\\ntext = \"Noritake Marywood China, Salad Plates #2181, 8.25\" White Platinum Blue, Raised Floral, Kitchen Dining, MINT Condition, 12 Available https://tuppu.net/6205a45 #Etsy #AmazingFunVintage\"\\n# å‘¼å«å‡½å¼åµæ¸¬ç¶²å€å’Œhashtag\\nurls, hashtags = detect_urls_and_hashtags(text)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_urls_hashtags_metions(text:str)-> str:\n",
    "    #print('** detect_urls_and_hashtags **')\n",
    "\n",
    "    # ç¶²å€çš„æ­£å‰‡è¡¨é”å¼\n",
    "    url_pattern = r'https?://\\S+'\n",
    "    \n",
    "    # hashtagçš„æ­£å‰‡è¡¨é”å¼\n",
    "    hashtag_pattern = r'#\\w+'\n",
    "    \n",
    "    # æåŠï¼ˆ@ï¼‰çš„æ­£å‰‡è¡¨é”å¼\n",
    "    mention_pattern = r'@\\w+'\n",
    "    \n",
    "    urls = re.findall(url_pattern, text)\n",
    "    \n",
    "    hashtags = re.findall(hashtag_pattern, text)\n",
    "    \n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    \n",
    "    # ç§»é™¤æ–‡å­—ä¸­çš„ç¶²å€\n",
    "    text_removed = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    return text_removed, len(urls), len(hashtags), len(mentions) ##########è¨ˆç®—æåŠ##########\n",
    "\n",
    "'''\n",
    "# instance\n",
    "text = \"Noritake Marywood China, Salad Plates #2181, 8.25\\\" White Platinum Blue, Raised Floral, Kitchen Dining, MINT Condition, 12 Available https://tuppu.net/6205a45 #Etsy #AmazingFunVintage\"\n",
    "# å‘¼å«å‡½å¼åµæ¸¬ç¶²å€å’Œhashtag\n",
    "urls, hashtags = detect_urls_and_hashtags(text)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29b298",
   "metadata": {},
   "source": [
    "### åµæ¸¬ä¸¦è½‰æ›emoji\n",
    "- åŒ…å«ç¹ªæ–‡å­—çš„è©±ä¸€æ¨£è¨ˆç®—å‡ºç¾æ¬¡æ•¸ï¼Œä¸¦å°‡ç¹ªæ–‡å­—è½‰å›æƒ…ç·’è©å½™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e69a931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ç¯„ä¾‹æ–‡å­—åŒ…å«è¡¨æƒ…ç¬¦è™Ÿ\\ntext = \"I\\'m feeling ğŸ˜€ and ğŸ˜\"\\ntext_with_names = detect_emoji(text)\\nprint(text_with_names)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_emoji(text:str)-> tuple:\n",
    "    #print('** detect_emoji **')\n",
    "    \n",
    "    #urls = re.findall(url_pattern, text)\n",
    "\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    emoji_count = sum(word.startswith(\":\") and word.endswith(\":\") for word in text)#############\n",
    "    \n",
    "    text_without_emoji = text.replace(\":\", \"\").replace(\"_face\", \"\")\n",
    "\n",
    "\n",
    "    # print(\"è½‰æ›å¾Œçš„æ–‡å­—:\", text_with_names)\n",
    "    return text_without_emoji, emoji_count\n",
    "\n",
    "'''\n",
    "# ç¯„ä¾‹æ–‡å­—åŒ…å«è¡¨æƒ…ç¬¦è™Ÿ\n",
    "text = \"I'm feeling ğŸ˜€ and ğŸ˜\"\n",
    "text_with_names = detect_emoji(text)\n",
    "print(text_with_names)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f62b367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_post_stability(df):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ¯å€‹æœˆçš„è²¼æ–‡æ•¸ä½”ç¸½è²¼æ–‡æ•¸çš„æ¯”ç‡ä»¥åŠç™¼æ–‡é »ç‡çš„ç©©å®šåº¦\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): åŒ…å«å¸–å­æ—¥æœŸçš„æ•¸æ“šæ¡†ï¼ˆDataFrameï¼‰\n",
    "\n",
    "    Returns:\n",
    "    months (list): æ¯å€‹æœˆä»½çš„åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰\n",
    "    post_ratios (list): æ¯å€‹æœˆä»½è²¼æ–‡æ•¸é‡ä½”ç¸½è²¼æ–‡æ•¸çš„æ¯”ç‡ï¼ˆæµ®é»æ•¸æ ¼å¼ï¼‰\n",
    "    post_stability (float): ç™¼æ–‡é »ç‡çš„ç©©å®šåº¦ï¼ˆæ¨™æº–å·®ï¼‰\n",
    "    total_posts (int): ç¸½è²¼æ–‡æ•¸\n",
    "    \"\"\"\n",
    "    # å°‡å¸–å­æ—¥æœŸè½‰æ›ç‚º datetime æ ¼å¼\n",
    "    df['postDate'] = pd.to_datetime(df['postDate'])\n",
    "    \n",
    "    # æå–å¹´ä»½å’Œæœˆä»½ï¼Œä¸¦æ·»åŠ æ–°çš„æ¬„ä½\n",
    "    df['year_month'] = df['postDate'].dt.to_period('M')\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹æœˆçš„å¸–å­æ•¸é‡\n",
    "    monthly_counts = df['year_month'].value_counts().sort_index()\n",
    "    \n",
    "    # è¨ˆç®—ç¸½å¸–å­æ•¸\n",
    "    total_posts = monthly_counts.sum()\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹æœˆä»½çš„å¸–å­æ•¸é‡ä½”ç¸½å¸–å­æ•¸çš„æ¯”ç‡\n",
    "    post_ratios = monthly_counts / total_posts\n",
    "    \n",
    "    # è¨ˆç®—ç™¼æ–‡é »ç‡çš„ç©©å®šåº¦ï¼ˆæ¨™æº–å·®ï¼‰\n",
    "    post_stability = np.std(post_ratios)\n",
    "    \n",
    "    # å°‡çµæœè½‰æ›ç‚ºå…©å€‹ list\n",
    "    months = [str(month) for month in post_ratios.index]  # å°‡æœˆä»½è½‰æ›ç‚ºå­—ç¬¦ä¸²æ ¼å¼\n",
    "\n",
    "    return months, post_ratios.tolist(), post_stability, total_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9594e8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## æ¨æ–‡ç¸½æ•¸\\ndef number_of_posts(df)->int:\\n    num_rows = df.shape[0]\\n    return num_rows\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## æ¨æ–‡ç¸½æ•¸\n",
    "def number_of_posts(df)->int:\n",
    "    num_rows = df.shape[0]\n",
    "    return num_rows\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f58f257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_ratios(textdata):\n",
    "    print('** def calculate_text_ratios **')\n",
    "\n",
    "    # è®¡ç®—ä½¿ç”¨hashtagçš„æ¯”ç‡\n",
    "    num_hashtags = textdata['numOfHashtags'].sum()\n",
    "    total_tweets = textdata.shape[0]\n",
    "    hashtag_ratio = num_hashtags / total_tweets\n",
    "\n",
    "    # è®¡ç®—ä½¿ç”¨emojiçš„æ¯”ç‡\n",
    "    num_emojis = textdata['numOfEmojis'].sum()\n",
    "    emoji_ratio = num_emojis / total_tweets\n",
    "\n",
    "    num_url = textdata['numOfUrls'].sum()\n",
    "    url_ratio = num_url / total_tweets\n",
    "    \n",
    "    num_mention = textdata['numOfMentions'].sum()\n",
    "    mention_ratio = num_url / total_tweets\n",
    "\n",
    "    return hashtag_ratio, emoji_ratio, url_ratio, mention_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776cb0d",
   "metadata": {},
   "source": [
    "## 3.3.2 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca771f1",
   "metadata": {},
   "source": [
    "## æ¸…æ´—æ–‡æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6eaba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map Penn Treebank POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(word:str):\n",
    "    #print('** get_wordnet_pos **')\n",
    "\n",
    "    # Get POS tag using nltk.pos_tag and map to WordNet POS\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wn.ADJ,  # Adjective\n",
    "                \"N\": wn.NOUN,  # Noun\n",
    "                \"V\": wn.VERB,  # Verb\n",
    "                \"R\": wn.ADV}   # Adverb\n",
    "    \n",
    "    return tag_dict.get(tag, wn.NOUN)  # Default to Noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e249a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(clean_text(\"\"I have 5 special invite codes available for  Join us and don\\'t forget to bring along your friends. Let\\'s embrace Zen! Oohm!\\\\ \" \" ))\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def text_preprocessing(text:str)->list:\n",
    "       \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  #åˆªå»æ¨™é»ç¬¦è™Ÿ\n",
    "    text = text.lower() # çµ±ä¸€è½‰ç‚ºå°å¯«\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text) # æ–·å¥ \n",
    "    tokens = [nltk.tokenize.word_tokenize(sent) for sent in sentences]  # æ–·è©\n",
    "    \n",
    "    nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens[0] if token not in nltk_stopwords] # åƒ…ä¿ç•™éåœç”¨å­—(å»é™¤åœç”¨å­—)\n",
    "        \n",
    "    return tokens # éœ€è¦å›å‚³list\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text:str)->str:\n",
    "    #print('** clean_text **')\n",
    "\n",
    "    # Remove non-English text\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    #text, url_count, hashtag_count = detect_urls_and_hashtags(text) # remove url and hashtags\n",
    "    #text = detect_emoji(text) # replace emoji to words\n",
    "    \n",
    "    # Remove emojis\n",
    "    #text = re.sub('[^\\w\\s,]', '', text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Lower Case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        pos = get_wordnet_pos(word)\n",
    "        if pos:\n",
    "            lemma = lemmatizer.lemmatize(word, pos)\n",
    "            lemmatized_words.append(lemma)\n",
    "    \n",
    "    # Join words back into text\n",
    "    cleaned_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "'''\n",
    "print(clean_text(\"\\\"I have 5 special invite codes available for  Join us and don't forget to bring along your friends. Let's embrace Zen! Oohm!\\ \\\" \" ))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e870a54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntweets = [\"Crypto is cool because it makes young people excited about the future, and that means something these days. Agree?\",\\n    \"her name is pretty.\",\\n \"dear @verizonsupport your service is straight ğŸ’© in dallas.. been with y\\'all over a decade and this is all time low for y\\'all. i\\'m talking no internet at all.\",\\n \"@verizonsupport I sent you a dm\",\\n \"thanks to michelle et al at @verizonsupport who helped push my no-show-phone problem along. Order canceled successfully, and I ordered this for pickup today at the Apple store in the mall.\"\\n ]\\npolarity, subjectivity = Sentiment_Analysis(tweets)\\n\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨ˆç®—æ¯ä¸€ç¯‡è²¼æ–‡çš„æƒ…ç·’ \n",
    "def Sentiment_Analysis(tweets:list):\n",
    "    print('** def Sentiment_Analysis **')\n",
    "    \n",
    "    polarity = []\n",
    "    subjectivity=[]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        blob = TextBlob(tweet)\n",
    "       # print(blob.sentiment) \n",
    "        polarity.append(blob.sentiment.polarity)  # polarity çš„å€¼åœ¨ç¯„åœ [-1, 1]ï¼Œè¡¨ç¤ºæƒ…æ„Ÿçš„æ­£è² ç¨‹åº¦\n",
    "        subjectivity.append(blob.sentiment.subjectivity)  #subjectivity çš„å€¼åœ¨ç¯„åœ [0, 1]ï¼Œè¡¨ç¤ºæ–‡æœ¬çš„ä¸»è§€æ€§ã€‚\n",
    "        print(f'Polarity: {blob.sentiment.polarity}, Subjectivity: {blob.sentiment.subjectivity}')\n",
    "        \n",
    "    return polarity, subjectivity\n",
    "    \n",
    "'''\n",
    "tweets = [\"Crypto is cool because it makes young people excited about the future, and that means something these days. Agree?\",\n",
    "    \"her name is pretty.\",\n",
    " \"dear @verizonsupport your service is straight ğŸ’© in dallas.. been with y'all over a decade and this is all time low for y'all. i'm talking no internet at all.\",\n",
    " \"@verizonsupport I sent you a dm\",\n",
    " \"thanks to michelle et al at @verizonsupport who helped push my no-show-phone problem along. Order canceled successfully, and I ordered this for pickup today at the Apple store in the mall.\"\n",
    " ]\n",
    "polarity, subjectivity = Sentiment_Analysis(tweets)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa776c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfile_path = 'textData/elonmusk.csv'\\ndf = pd.read_csv(file_path)\\ncount_rate_of_pos_Sentiment(df)\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_rate_of_pos_Sentiment(df)->float:\n",
    "    print('** def count_rate_of_pos_Sentimentt **')\n",
    "    \n",
    "    numOfTweet = len(df)  # çµ±è¨ˆç¸½æ¨æ–‡æ•¸é‡\n",
    "    PositiveTweet = df.loc[df['sentiment'] > 0]  \n",
    "    numOfPositive = len(PositiveTweet)  # çµ±è¨ˆæ­£å‘æ¨æ–‡æ•¸é‡\n",
    "    rate_of_post_Sentiment = numOfPositive / numOfTweet  # è¨ˆç®—æ­£å‘æ¨æ–‡ä½”æ¯”\n",
    "    \n",
    "    print(f'æ­£è²¼æ–‡æ¯”: {rate_of_post_Sentiment}')\n",
    "    return rate_of_post_Sentiment\n",
    "\n",
    "\n",
    "'''\n",
    "file_path = 'textData/elonmusk.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "count_rate_of_pos_Sentiment(df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c0661d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_avg_Subjectivity(textdata):\n",
    "    \n",
    "    return textdata['subjectivity'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a083b8",
   "metadata": {},
   "source": [
    "## 3.3.3\tTopic Analysis\n",
    "GENSIM document: https://radimrehurek.com/gensim/models/ldamulticore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5dd5a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndocuments = [\\n    \"topic modeling is interesting and useful\",\\n    \"gensim library provides tools for topic modeling\",\\n    \"LDA is a popular algorithm for topic modeling\",\\n    \"topic modeling helps in discovering hidden patterns in text data\",\\n    \"NLP techniques are often used in topic modeling\"\\n]\\n\\nnum_topics = 2\\nlda_model = train_LDA_model(documents, num_topics)\\n\\nprint(\"LDA Topics:\")\\nprint(lda_model.print_topics())\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_LDA_model(documents:list, num_topics:int):\n",
    "    print('** def train_LDA_model **')\n",
    "\n",
    "    # æ–·è©\n",
    "    texts = [[word for word in document.split()] for document in documents]\n",
    "\n",
    "    # å»ºè©å…¸\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # å»ºè©é¢‘çŸ©é™£\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # train LDA model\n",
    "    #lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=20) #é€™å€‹é€Ÿåº¦æ¯”è¼ƒæ…¢\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics)\n",
    "\n",
    "    #print(lda_model.print_topics())\n",
    "    return lda_model\n",
    "\n",
    "'''\n",
    "documents = [\n",
    "    \"topic modeling is interesting and useful\",\n",
    "    \"gensim library provides tools for topic modeling\",\n",
    "    \"LDA is a popular algorithm for topic modeling\",\n",
    "    \"topic modeling helps in discovering hidden patterns in text data\",\n",
    "    \"NLP techniques are often used in topic modeling\"\n",
    "]\n",
    "\n",
    "num_topics = 2\n",
    "lda_model = train_LDA_model(documents, num_topics)\n",
    "\n",
    "print(\"LDA Topics:\")\n",
    "print(lda_model.print_topics())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46cc3d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfile_path = 'Airdrops0637.csv'\\ndf = pd.read_csv(file_path)\\ncreator_text = df['CleanedText'].tolist()\\ndocument_topics = topic_analysis(documents=creator_text, lda_model=lda_model)\\n#print(document_topics)\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topic_analysis(documents:list, lda_model):\n",
    "   # print('** def topic_analysis **')\n",
    "    document_topics = []\n",
    "    for document in documents:\n",
    "        # åˆ†è¯å¤„ç†\n",
    "        words = [word for word in document.split()]\n",
    "        # å°†æ–‡æ¡£è½¬æ¢ä¸ºè¯è¢‹è¡¨ç¤º\n",
    "        document_bow = lda_model.id2word.doc2bow(words)\n",
    "        # è·å–æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ\n",
    "        topics = lda_model.get_document_topics(document_bow)\n",
    "        # é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ä¸»é¢˜ä½œä¸ºåˆ†ç±»æ ‡ç­¾\n",
    "        if topics:\n",
    "            dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            dominant_topic = None\n",
    "        document_topics.append(dominant_topic)\n",
    "    return document_topics\n",
    "\n",
    "\n",
    "'''\n",
    "file_path = 'Airdrops0637.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "creator_text = df['CleanedText'].tolist()\n",
    "document_topics = topic_analysis(documents=creator_text, lda_model=lda_model)\n",
    "#print(document_topics)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77244b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntopic_tag = [2, 8, 4, 2, 6, 4, 9, 0, 1, 5, 0, 7, 6, 0, 5, 3, 6, 7, 1, 6, 3, 0, 9, 7, 4, 1, 2, 1, 0, 0, 4, 2, 3, 6, 1, 3, 2, 8, 2, 6, 5, 0, 8, 4, 5, 6, 1, 1, 9, 9, 5, 4, 4, 1, 1, 8, 4, 1, 6, 7, 5, 6, 8, 5, 1, 0, 8, 6, 5, 1, 2, 0, 4, 8, 3, 2, 9, 4, 6, 6, 8, 1, 6, 4, 9, 2, 4, 6, 7, 0, 6, 4, 5, 8, 7, 6, 0, 9, 5, 8, 6, 1, 8, 0, 8, 1, 5, 1, 1, 3, 4, 3, 4, 3, 3, 6, 0, 3, 4, 3, 4, 3, 5, 8, 5, 8, 3, 8, 6, 2, 2, 1, 9, 1, 6, 5, 8, 0, 2, 8, 4, 5, 8, 6, 1, 4, 8, 5, 5, 8, 0, 6, 5, 1]\\nposts_per_topic = count_posts_per_topic(topic_tag=topic_tag, num_of_topics=10)\\n\\nprint(posts_per_topic)\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def count_posts_per_topic(topic_tag: list, num_of_topics: int):\n",
    "    print('** def count_posts_per_topic **')\n",
    "    \n",
    "    posts_per_topic = {}\n",
    "    for topic_index in range(num_of_topics):  \n",
    "        count = topic_tag.count(topic_index)\n",
    "        topic_name = f'topic{topic_index+1}'# ä¸»é¡Œç´¢å¼•å¾1é–‹å§‹\n",
    "        posts_per_topic[topic_name] = count/len(topic_tag)\n",
    "    df = pd.DataFrame.from_dict(posts_per_topic, orient='index', columns=['Count'])\n",
    "    \n",
    "    return df #posts_per_topic \n",
    "\n",
    "'''\n",
    "\n",
    "topic_tag = [2, 8, 4, 2, 6, 4, 9, 0, 1, 5, 0, 7, 6, 0, 5, 3, 6, 7, 1, 6, 3, 0, 9, 7, 4, 1, 2, 1, 0, 0, 4, 2, 3, 6, 1, 3, 2, 8, 2, 6, 5, 0, 8, 4, 5, 6, 1, 1, 9, 9, 5, 4, 4, 1, 1, 8, 4, 1, 6, 7, 5, 6, 8, 5, 1, 0, 8, 6, 5, 1, 2, 0, 4, 8, 3, 2, 9, 4, 6, 6, 8, 1, 6, 4, 9, 2, 4, 6, 7, 0, 6, 4, 5, 8, 7, 6, 0, 9, 5, 8, 6, 1, 8, 0, 8, 1, 5, 1, 1, 3, 4, 3, 4, 3, 3, 6, 0, 3, 4, 3, 4, 3, 5, 8, 5, 8, 3, 8, 6, 2, 2, 1, 9, 1, 6, 5, 8, 0, 2, 8, 4, 5, 8, 6, 1, 4, 8, 5, 5, 8, 0, 6, 5, 1]\n",
    "posts_per_topic = count_posts_per_topic(topic_tag=topic_tag, num_of_topics=10)\n",
    "\n",
    "print(posts_per_topic)\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e6dbd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ndef append_num_of_topic_to_csv(posts_per_topic: dict, csv_filename: str):\\n    print('** def append_num_of_topic_to_csv **')\\n    \\n    # å°‡å­—å…¸è½‰æ›ç‚º DataFrame\\n    df = pd.DataFrame.from_dict(posts_per_topic, orient='index', columns=['Value'])\\n    \\n    # å°‡ DataFrame å¯«å…¥ CSV æ–‡ä»¶ï¼Œä¸¦æŒ‡å®š index_label ç‚ºç´¢å¼•åˆ—\\n    df.to_csv(csv_filename, mode='a', header=False, index_label='Index')\\n   \\n    return\\n   \\nappend_num_of_topic_to_csv(posts_per_topic=posts_per_topic, csv_filename='text.csv')\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "def append_num_of_topic_to_csv(posts_per_topic: dict, csv_filename: str):\n",
    "    print('** def append_num_of_topic_to_csv **')\n",
    "    \n",
    "    # å°‡å­—å…¸è½‰æ›ç‚º DataFrame\n",
    "    df = pd.DataFrame.from_dict(posts_per_topic, orient='index', columns=['Value'])\n",
    "    \n",
    "    # å°‡ DataFrame å¯«å…¥ CSV æ–‡ä»¶ï¼Œä¸¦æŒ‡å®š index_label ç‚ºç´¢å¼•åˆ—\n",
    "    df.to_csv(csv_filename, mode='a', header=False, index_label='Index')\n",
    "   \n",
    "    return\n",
    "   \n",
    "append_num_of_topic_to_csv(posts_per_topic=posts_per_topic, csv_filename='text.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f03ff81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ä½¿ç”¨ç¯„ä¾‹\\ntopic_folder = \"dataset/allTextTopic\"\\nuser_folder = \"dataset/userdata\"\\nget_topic_and_write(topic_folder, user_folder)\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_topic_and_write(topic_folder: str, user_folder: str):\n",
    "    # æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(topic_folder) or not os.path.exists(user_folder):\n",
    "        print(\"æŒ‡å®šçš„è³‡æ–™å¤¾ä¸å­˜åœ¨ã€‚\")\n",
    "        return\n",
    "    \n",
    "    # éæ­·ä¸»é¡Œè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    for topic_file in os.listdir(topic_folder):\n",
    "        \n",
    "        # è®€å–ä¸»é¡Œ CSV æ–‡ä»¶\n",
    "        try:\n",
    "            topic_path = os.path.join(topic_folder, topic_file)\n",
    "            topic_df = pd.read_csv(topic_path)\n",
    "        except Exception as e:\n",
    "            print(\"ç„¡æ³•è®€å–ä¸»é¡Œæ–‡ä»¶:\", e)\n",
    "            continue\n",
    "        try:\n",
    "            user_path = os.path.join(user_folder, topic_file)\n",
    "            user_path = user_path.replace('.csv','_bg.csv')\n",
    "            user_df = pd.read_csv(user_path)\n",
    "            \n",
    "            \n",
    "            # æ‰¾åˆ°ä¸»é¡Œ\n",
    "            topic_count = topic_df['Count']\n",
    "            idx = np.where(topic_count == 1)[0]\n",
    "            topic = topic_count.index[idx[0]]\n",
    "        \n",
    "            print(f'-----{topic}-----')\n",
    "            # å°‡ä¸»é¡Œè³‡è¨Šæ‡‰ç”¨åˆ°ä½¿ç”¨è€… DataFrame ä¸­\n",
    "            user_df['creator_topic'] = topic\n",
    "            \n",
    "            # ä¿å­˜ä¿®æ”¹å¾Œçš„ä½¿ç”¨è€… DataFrame\n",
    "            user_df.to_csv(user_path, index=False)\n",
    "            print(f\"å·²æˆåŠŸå°‡ä¸»é¡Œè³‡è¨Šæ‡‰ç”¨åˆ°ä½¿ç”¨è€…æ–‡ä»¶ {user_path} ä¸­ä¸¦ä¿å­˜ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"ç„¡æ³•ä¿å­˜ä½¿ç”¨è€…æ–‡ä»¶ {user_path}:\", e)\n",
    "'''\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "topic_folder = \"dataset/allTextTopic\"\n",
    "user_folder = \"dataset/userdata\"\n",
    "get_topic_and_write(topic_folder, user_folder)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6bced5",
   "metadata": {},
   "source": [
    "## 3.4.1\tPersonality Trait Analysis\n",
    "By æ±ç©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4d92a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Penn Treebank POS tag to WordNet POS tag\n",
    "def ptb_to_wn(tag):  \n",
    "    #print('** def ptb_to_wn **')\n",
    "\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    return None  # Return None for other cases\n",
    "\n",
    "# Function to convert tagged word to WordNet synset\n",
    "def tagged_to_synset(word, tag):\n",
    "    #print('** def tagged_to_synset **')\n",
    "    \n",
    "    wn_tag = ptb_to_wn(tag)  # Convert Penn Treebank POS tag to WordNet POS tag\n",
    "    if wn_tag is None:\n",
    "        return None  # Return None if POS tag is not recognized \n",
    "    try:\n",
    "        # Get the first synset for the word and POS tag\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None  # Return None if no synsets are found\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf3e206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the similarity score between two sentences\n",
    "def sentence_similarity(s1, s2):    \n",
    "    #print('** def sentence_similarity **')\n",
    "    \n",
    "    # Tokenize and POS tag the input sentences\n",
    "    s1 = pos_tag(word_tokenize(s1))\n",
    "    s2 = pos_tag(word_tokenize(s2)) \n",
    "\n",
    "    # Convert POS-tagged words to WordNet synsets\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in s1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in s2]\n",
    "\n",
    "    # Remove \"None\" values from synsets\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "\n",
    "    score, count = 0.0, 0\n",
    "\n",
    "    # Calculate the similarity score for each synset in the first sentence\n",
    "    for synset in synsets1:\n",
    "        try:\n",
    "            # Find the best similarity score with synsets in the second sentence\n",
    "            best_score = max([synset.path_similarity(ss) for ss in synsets2])\n",
    "            \n",
    "            if best_score is not None:\n",
    "                score += best_score\n",
    "                count += 1\n",
    "        except:\n",
    "            score = 0  # Handle exceptions by setting score to 0\n",
    "\n",
    "    try:\n",
    "        score /= count  # Calculate the average score\n",
    "    except ZeroDivisionError:\n",
    "        score = 0  # Set score to 0 if there are no valid scores\n",
    "\n",
    "    return score\n",
    "\n",
    "# Function to compute symmetric sentence similarity\n",
    "def symSentSim(s1, s2):\n",
    "    #print('** def symSentSim **')\n",
    "    # Calculate the symmetric sentence similarity score\n",
    "    sss_score = (sentence_similarity(s1, s2) + sentence_similarity(s2, s1)) / 2\n",
    "    return sss_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5bc53b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\ndf = pd.read_csv('chelice_bts.csv')\\npersonality_analysis(df['textresponding'], 'chelice_bts.csv')\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extraversion = ['active', 'adventurous', 'affectionate', 'assertive', 'boisterous', 'bold', 'cheerful', 'confident', 'dominant', 'energetic', 'enthusiastic', 'extroverted', 'friendly', 'gregarious', 'impulsive', 'loud', 'outgoing', 'playful', 'self-assured', 'sociable', 'spontaneous', 'talkative', 'thrill-seeking', 'vivacious', 'warm', 'fun-loving', 'risk-taking', 'party-loving', 'stimulation-seeking', 'high-spirited', 'life-of-the-party', 'people-person', 'social-butterfly', 'charismatic', 'daring', 'gregariousness', 'assertiveness', 'excitement-seeking', 'positive-affect', 'activity', 'optimism', 'happiness', 'affiliation', 'extraversion', 'outgoingness', 'sensation-seeking', 'cheerfulness', 'friendliness']\n",
    "# agreeableness = ['affectionate', 'appreciative', 'caring', 'compassionate', 'considerate', 'cooperative', 'empathetic', 'forgiving', 'generous', 'helpful', 'kind', 'likable', 'nurturing', 'pleasant', 'polite', 'sensitive', 'sweet', 'sympathetic', 'thoughtful', 'tolerant', 'trusting', 'warm', 'altruistic', 'charitable', 'friendly', 'unselfish', 'gentle', 'tender', 'good-natured', 'cooperative', 'agreeableness', 'empathy', 'love', 'teamwork', 'modesty', 'generosity', 'consideration', 'kindness', 'pleasantness', 'gentleness', 'cooperation', 'affiliation', 'interpersonal-sensitivity', 'warmth', 'helpfulness', 'trust', 'compassion', 'friendliness']\n",
    "# conscientiousness = ['competent', 'confident', 'determined', 'disciplined', 'dutiful', 'efficient', 'focused', 'goal-oriented', 'hard-working', 'organized', 'punctual', 'reliable', 'responsible', 'self-disciplined', 'thorough', 'thoughtful', 'ambitious', 'self-controlled', 'dependable', 'industrious', 'productive', 'persistent', 'practical', 'structured', 'systematic', 'orderly', 'methodical', 'achievement-striving', 'cautiousness', 'dutifulness', 'orderliness', 'achievement', 'responsibility', 'deliberation', 'dependability', 'organization', 'perseverance', 'self-efficacy', 'self-discipline', 'conscientiousness', 'work-ethic', 'planning', 'perseverance', 'industriousness', 'accuracy', 'detail-orientation', 'reliability', 'goal-directedness']\n",
    "# neuroticism = ['anxious', 'apprehensive', 'depressed', 'emotional', 'fearful', 'frustrated', 'gloomy', 'guilty', 'harassed', 'hopeless', 'hysterical', 'insecure', 'irritable', 'jittery', 'lonesome', 'melancholy', 'nervous', 'panicky', 'paralyzed', 'paranoid', 'perturbed', 'pessimistic', 'sad', 'scared', 'shaky', 'startled', 'suspicious', 'tense', 'terrified', 'threatened', 'timid', 'troubled', 'unhappy', 'vulnerable', 'weepy', 'worried', 'fearfulness', 'anxiety', 'anger', 'depression', 'self-consciousness', 'immoderation', 'vulnerability', 'stress', 'self-doubt', 'rumination', 'negativity', 'moodiness', 'fear', 'hostility', 'shame', 'insecurity', 'hypersensitivity', 'emotional-reactivity', 'worry', 'hopelessness', 'neuroticism']\n",
    "# openness = ['artistic', 'creative', 'curious', 'imaginative', 'inventive', 'original', 'unconventional', 'wide interests', 'open-minded', 'broad-minded', 'innovative', 'intelligent', 'knowledgeable', 'perceptive', 'philosophical', 'reflective', 'sophisticated', 'unconventional', 'visionary', 'intellectual', 'wisdom', 'aesthetic-appreciation', 'emotional-awareness', 'fantasy', 'creativity', 'openness-to-experience', 'divergent-thinking', 'interests', 'imagination', 'abstract-thinking', 'complexity', 'originality', 'intellect', 'unconventionality', 'appreciation-for-art', 'insightfulness', 'liberalism', 'openness']\n",
    "def personality_analysis(df, file_path:str):\n",
    "    print('** def personality_analysis **')\n",
    "    \n",
    "    try:\n",
    "        extraversion = ['talkative', 'assertive', 'enthusiasm', 'energetic', 'adventure', 'dominance', 'social', 'excitement', 'fun', 'optimism']\n",
    "        agreeableness = ['politeness', 'helpful', 'kind', 'empathy', 'cooperation', 'modesty', 'affection', 'sympathy', 'pleasant', 'trust']\n",
    "        conscientiousness = ['achievement', 'striving', 'planning', 'organized', 'dutiful', 'discipline', 'work', 'responsible', 'dependable', 'perseverance']\n",
    "        neuroticism = ['anxiety', 'anger', 'depression', 'emotional', 'stress', 'vulnerability', 'fear', 'nervous', 'tense', 'worry']\n",
    "        openness = ['insight', 'curious', 'interest', 'imagination', 'unconventional', 'originality', 'creativity', 'art', 'novel', 'idea']\n",
    "        o = ' '.join(openness)\n",
    "        c = ' '.join(conscientiousness)\n",
    "        e = ' '.join(extraversion)\n",
    "        a = ' '.join(agreeableness)\n",
    "        n = ' '.join(neuroticism)\n",
    "\n",
    "\n",
    "    \n",
    "        #df['CleanedText'] = df['textWithoutEmoji'].apply(clean_text)\n",
    "\n",
    "        # æ¯”å°è©å½™æ„ç¾©ç›¸ä¼¼åº¦\n",
    "        df['O_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, o)) \n",
    "        df['C_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, c))\n",
    "        df['E_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, e))\n",
    "        df['A_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, a))\n",
    "        df['N_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, n))\n",
    "        print('Get OCEAN Score success!')\n",
    "    except Exception as e:\n",
    "        print(\"è¨ˆç®—å¤§äº”äººæ ¼åˆ†æ•¸æ™‚å‡ºç¾éŒ¯èª¤:\", e)\n",
    "    \n",
    "    try:\n",
    "        big5_file_path = file_path.replace('.csv','_big5.csv')#save(new file)\n",
    "        df.to_csv(big5_file_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"ç„¡æ³•æ­£ç¢ºä¿å­˜å¤§äº”äººæ ¼åˆ†æ•¸:\", e)\n",
    "\n",
    "'''    \n",
    "df = pd.read_csv('chelice_bts.csv')\n",
    "personality_analysis(df['textresponding'], 'chelice_bts.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba458d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# è°ƒç”¨å‡½æ•°å¹¶è¾“å‡º OCEAN å„è‡ªçš„å¹³å‡åˆ†æ•°\\nuser_file_path = 'seokjinism1_bg.csv'\\ntext_file_path = 'seokjinism1.csv'  # è¯·æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\\ncalculate_ocean_avg_scores(user_file_path,text_file_path)\\n\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_ocean_avg_scores(user_file_path: str, text_file_path:str):\n",
    "    print('** def calculate_ocean_avg_scores **')\n",
    "\n",
    "    try:\n",
    "        # ä»æ–‡ä»¶ä¸­è¯»å–æ•°æ®\n",
    "        userFile = pd.read_csv(user_file_path)\n",
    "        textFile = pd.read_csv(text_file_path)\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„å¹³å‡åˆ†æ•°\n",
    "        userFile['O_Score'] = [textFile['O_Score'].mean()]\n",
    "        userFile['C_Score'] = [textFile['C_Score'].mean()]\n",
    "        userFile['E_Score'] = [textFile['E_Score'].mean()]\n",
    "        userFile['A_Score'] = [textFile['A_Score'].mean()]\n",
    "        userFile['N_Score'] = [textFile['N_Score'].mean()]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"è®¡ç®— OCEAN å¹³å‡åˆ†æ•°æ—¶å‡ºç°é”™è¯¯:\", e)\n",
    "\n",
    "    try:\n",
    "        userFile.to_csv(user_file_path, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"å­˜æª”é”™è¯¯:\", e)\n",
    "\n",
    "'''\n",
    "# è°ƒç”¨å‡½æ•°å¹¶è¾“å‡º OCEAN å„è‡ªçš„å¹³å‡åˆ†æ•°\n",
    "user_file_path = 'seokjinism1_bg.csv'\n",
    "text_file_path = 'seokjinism1.csv'  # è¯·æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\n",
    "calculate_ocean_avg_scores(user_file_path,text_file_path)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "800fcdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# è°ƒç”¨å‡½æ•°å¹¶è¾“å‡º OCEAN å„è‡ªçš„å¹³å‡åˆ†æ•°\\nuser_file_path = 'seokjinism1_bg.csv'\\ntext_file_path = 'seokjinism1.csv'  # è¯·æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\\ncalculate_ocean_avg_scores(user_file_path,text_file_path)\\n\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_follower_ocean_avg_scores(user_file_path: str, text_file_path:str):\n",
    "    print('** def calculate_ocean_avg_scores **')\n",
    "\n",
    "    try:\n",
    "        # ä»æ–‡ä»¶ä¸­è¯»å–æ•°æ®\n",
    "        userFile = pd.read_csv(user_file_path)\n",
    "        textFile = pd.read_csv(text_file_path)\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„å¹³å‡åˆ†æ•°\n",
    "        userFile['Follower_O_Score'] = [textFile['O_Score'].mean()]\n",
    "        userFile['Follower_C_Score'] = [textFile['C_Score'].mean()]\n",
    "        userFile['Follower_E_Score'] = [textFile['E_Score'].mean()]\n",
    "        userFile['Follower_A_Score'] = [textFile['A_Score'].mean()]\n",
    "        userFile['Follower_N_Score'] = [textFile['N_Score'].mean()]\n",
    "        print(textFile['N_Score'].mean())\n",
    "    except Exception as e:\n",
    "        print(\"è®¡ç®— OCEAN å¹³å‡åˆ†æ•°æ—¶å‡ºç°é”™è¯¯:\", e)\n",
    "\n",
    "    try:\n",
    "        userFile.to_csv(user_file_path, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"å­˜æª”é”™è¯¯:\", e)\n",
    "\n",
    "'''\n",
    "# è°ƒç”¨å‡½æ•°å¹¶è¾“å‡º OCEAN å„è‡ªçš„å¹³å‡åˆ†æ•°\n",
    "user_file_path = 'seokjinism1_bg.csv'\n",
    "text_file_path = 'seokjinism1.csv'  # è¯·æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\n",
    "calculate_ocean_avg_scores(user_file_path,text_file_path)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071428f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0b5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
