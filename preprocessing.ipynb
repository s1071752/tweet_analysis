{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aedb3c9",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "這裡會存放所有資料前處理及資料分析的function\n",
    "\n",
    "\n",
    "### 檔案說明\n",
    "會使用到raw data，然後這裡做分析取得指標之後會寫回raw data(會更新原檔)，所以有需要保留原檔案的話請自行備份rawdata(或使用函數copy_rawData)\n",
    "\n",
    "### 程式邏輯\n",
    "- 每一個function都只做一件事(例如我要計算參與度並且把參與度寫回csv，我會用兩個函數去做這件事，A函數算參與度，B函數寫檔案)\n",
    "- 每個函數的參數都已經標註資料型態，資料型態不對的話跑不動這個需要注意一下\n",
    "- 部分重複執行的函數我把拿來當作log的print註解掉了(不然會out of memory)\n",
    "- funtion下面會有範例的輸入，可以拿來測試看看"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93148d1b",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc869db",
   "metadata": {},
   "source": [
    "create_folder: 備份檔案用，檢查資料夾是否存在，如果不存在就創建  \n",
    "copy_rawData: 備份檔案用，會複製單一csv檔案到另一個目錄之中  \n",
    "create_txt_file: 開小筆記紀錄一些指標用的。若檔案存在就開檔直接繼續新增資料到下一行，不存在則創建檔案  \n",
    "read_csv_files_in_directory: 讀取放在目錄之下的所有csv檔案(所以我會把同類型的檔案整理到同一個目錄下做批次處理)  \n",
    "get_Col_data: 把需要用到的數個特徵資料列的資料撈出來(因為分析時通常會需要很多特徵)  \n",
    "Add_Collumn_to_file: 把計算得出的結果加回創作者的資料檔之中(最後預測會用這個檔案)  \n",
    "count_creator_engagement: 計算創作者參與度(公式=參與/粉絲數)  \n",
    "count_engagement_Threshold: creator_engagement取平均得到engagement_Threshold  \n",
    "judje_creator_success: 根據creator_engagement與engagement_Threshold判斷是否成功    \n",
    "str_To_Number: 做follower的前處理  \n",
    "Classify_influencerType: 根據網紅定義分類  \n",
    "\n",
    "\n",
    "detect_urls_hashtags_metions: 檢測此作者的文章是否有hashtag、網址，有的話標註包含次數在檔案中  \n",
    "detect_emoji   \n",
    "calculate_monthly_post_stability: 計算發文穩定度   \n",
    "text_preprocessing: 包含繪文字的話一樣計算出現次數，並將繪文字轉回情緒詞彙  \n",
    "get_wordnet_pos: 前處理用。Function to map Penn Treebank POS tag to WordNet POS tag  \n",
    "clean_text: 前處理用。移除無法分析的字元並做斷詞  \n",
    "Sentiment_Analysis: 計算每一篇貼文的情緒  \n",
    "count_rate_of_post_Sentiment: 計算正向情感貼文的比率  \n",
    "count_avg_Subjectivity  \n",
    "\n",
    "\n",
    "train_LDA_model  \n",
    "topic_analysis  \n",
    "get_topic_and_write  \n",
    "symSentSim: 比對大五人格辭典計算OCEAN五大項目得分  \n",
    "personality_analysis: 將分數寫入creator_big5.csv檔案中  \n",
    "calculate_ocean_avg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c85c1",
   "metadata": {},
   "source": [
    "## 引用套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f0e3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取檔案\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b480cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Nature Language Tool Kit (NLTK)處理文本\n",
    "import nltk  \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12ebb26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('punkt')\\nnltk.download('averaged_perceptron_tagger')\\nnltk.download('vader_lexicon')\\nnltk.download('stopwords')\\nnltk.download('wordnet')\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "421db68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本處理\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59a5a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "904872c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa7205",
   "metadata": {},
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca03df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查資料夾是否存在，如果不存在就創建資料夾\n",
    "def create_folder(folder_path):\n",
    "    print('** def create_folder **')\n",
    "    try:\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"資料夾 '{folder_path}' 創建成功！\")\n",
    "        else:\n",
    "            print(f\"資料夾 '{folder_path}' 已經存在。\")\n",
    "    except Exception as e:\n",
    "        print(f\"創建資料夾時發生錯誤：{e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "# 保存備份raw data\n",
    "def copy_rawData(source_file_path:str, target_file_path:str):\n",
    "    print('** def copy_rawData **')\n",
    "    # 複製單一csv檔案到另一個目錄之中\n",
    "    try:\n",
    "        shutil.copy(source_file_path, target_file_path)\n",
    "        #另一種備份方法:\n",
    "        #df = pd.read_csv(source_file_path)\n",
    "        #df.to_csv(target_file_path, index=False)\n",
    "        print('備份原檔案成功')\n",
    "    except:\n",
    "        print('備份原檔案時發生錯誤')\n",
    " \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "#new_folder = 'dataset\\userdata\\copy'\n",
    "#create_folder(new_folder)\n",
    "#copy_rawData(source_file_path='userData/Azuki_bg.csv', target_file_path= new_folder+'/Azuki_bg.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8aa0d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單記個txt筆記\n",
    "from datetime import datetime\n",
    "\n",
    "def create_txt_file(contents:str, txt_file_name:str):\n",
    "    print('** def create_txt_file**')\n",
    "\n",
    "    try:\n",
    "        file_path = txt_file_name + '.txt'\n",
    "        with open(file_path, 'a+') as file:\n",
    "            # 獲取當前日期和時間\n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            # 在每次新增內容時換行，並標註寫檔日期和時間\n",
    "            file.write(f\"\\n{current_time}\\n{contents}\")\n",
    "        print('成功寫入新資訊', contents, '至TXT檔案', txt_file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"寫TXT檔案時出現錯誤: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf77686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# 輸入指定的目錄路徑\\ndirectory_path = \"dataset/textData\"\\nread_csv_files_in_directory(directory_path)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv_files_in_directory(directory_path:str):  \n",
    "    print('** def read_csv_files_in_directory**')\n",
    "    # 檢查目錄是否存在\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"目錄 '{directory_path}' 不存在\")\n",
    "        return\n",
    "    \n",
    "    # 遍歷目錄中的所有檔案和子目錄\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            # 確保檔案是 CSV 格式\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"正在讀取檔案: {file_path}\")\n",
    "                \n",
    "                # 使用 pandas 讀取 CSV 檔案\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    # 在這裡可以對資料進行處理\n",
    "                   # print(df.head())  # 這裡只是示例，顯示檔案的前幾行\n",
    "                except Exception as e:\n",
    "                    print(f\"讀取檔案時出現錯誤: {e}\")\n",
    "'''\n",
    "\n",
    "# 輸入指定的目錄路徑\n",
    "directory_path = \"dataset/textData\"\n",
    "read_csv_files_in_directory(directory_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eab0e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Col_data(df, featureCols: list)-> dict:\n",
    "    print('** def get_Col_data **')\n",
    "    feature_col_dict = {}\n",
    "    for colName in featureCols:\n",
    "        try:\n",
    "            # 使用 loc 函數選取特定列的所有值\n",
    "            feature_values = df[colName].tolist()\n",
    "            # 將特徵值打包成列表並添加到字典中\n",
    "            feature_col_dict.update({colName:feature_values})\n",
    "        except KeyError:\n",
    "            print(f\"未找到列名 {colName}\")\n",
    "    return feature_col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b53c7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAdd_Collumn_to_file(New_Collumn_Name='Col',New_Collumn_list=['test'], csv_file_path='chelice_bts.csv' )\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Add_Collumn_to_file(New_Collumn_Name:str,  New_Collumn_list:list, csv_file_path:str):\n",
    "    print('** def Add_Collumn_to_file **')\n",
    "    \n",
    "    # 開啟舊的 CSV 檔案\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"找不到檔案: {csv_file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"讀取檔案時出現錯誤: {e}\")\n",
    "        return\n",
    "    df[New_Collumn_Name] = New_Collumn_list\n",
    "\n",
    "\n",
    "\n",
    "    # 指定您想要保存的檔案路徑\n",
    "    output_file_path = csv_file_path\n",
    "\n",
    "# 將更新後的 DataFrame 寫入 CSV 檔案\n",
    "    try:\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "        print(\"DataFrame 已成功保存到檔案:\", output_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"保存 DataFrame 到檔案時出現錯誤:\", e)\n",
    "    \n",
    "\n",
    "'''\n",
    "Add_Collumn_to_file(New_Collumn_Name='Col',New_Collumn_list=['test'], csv_file_path='chelice_bts.csv' )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515190d0",
   "metadata": {},
   "source": [
    "# 3.2 Creator Success Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49c10509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_creator_engagement(retweets, comments, likes, followers:int=1) -> float:\n",
    "    print('** def count_creator_engagement **')\n",
    "    \n",
    "#    retweets = strList_To_NumberList(list_with_comma=retweets)\n",
    "#    comments = strList_To_NumberList(list_with_comma=comments)\n",
    "#    likes = strList_To_NumberList(list_with_comma=likes)\n",
    "    \n",
    "    # 計算創作者參與度\n",
    "    creator_engagement = (sum(retweets) + sum(comments) + sum(likes)) / followers /len(likes)\n",
    "    \n",
    "    return creator_engagement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f9f1494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nengagement_Threshold = count_engagement_Threshold('dataset/userdata')\\ncreate_txt_file(contents=str(engagement_Threshold), txt_file_name='count_engagement_Threshold')\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_engagement_Threshold(directory_path)-> float:\n",
    "    print('** def count_engagement_Threshold **')\n",
    "    \n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"目錄 '{directory_path}' 不存在\")\n",
    "        return\n",
    "    \n",
    "    creator_engagement = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"正在讀取檔案: {file_path}\")\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    creator_engagement.append(df['creator_engagement'].iloc[0]) \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"讀取檔案時出現錯誤: {e}\")\n",
    "                    \n",
    "                    \n",
    "    creator_engagement_mean = sum(creator_engagement)/len(creator_engagement) #取平均\n",
    "    print('engagement_Threshold: ',creator_engagement_mean)\n",
    "    return creator_engagement_mean\n",
    "\n",
    "'''\n",
    "engagement_Threshold = count_engagement_Threshold('dataset/userdata')\n",
    "create_txt_file(contents=str(engagement_Threshold), txt_file_name='count_engagement_Threshold')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65d74295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\ndf = pd.read_csv('airasia.csv')\\nans = judje_creator_success(creator_engagement=df['creator_engagement'].iloc[0], engagement_Threshold=24415.11111111111)    \\nprint(ans)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def judje_creator_success(creator_engagement:float, engagement_Threshold: float):\n",
    "    if creator_engagement>= engagement_Threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "'''    \n",
    "df = pd.read_csv('airasia.csv')\n",
    "ans = judje_creator_success(creator_engagement=df['creator_engagement'].iloc[0], engagement_Threshold=24415.11111111111)    \n",
    "print(ans)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3088f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstr_To_Number('1,059K')\\nstr_To_Number('1,000')\\nstr_To_Number('2.9K')\\nstr_To_Number('19K')\\nstr_To_Number('3,900K')\\nstr_To_Number('3.1M')\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str_To_Number(value_with_comma:str)->int:\n",
    "    print('** def str_To_Number **')\n",
    "    print(f'value_with_comma:{value_with_comma}')\n",
    "    \n",
    "    if ',' in value_with_comma:\n",
    "        value_with_comma = value_with_comma.replace(',', '')\n",
    "    if 'K' in value_with_comma :\n",
    "        value_with_comma = value_with_comma.replace('K','000') \n",
    "    \n",
    "    if 'M' in value_with_comma:\n",
    "        value_with_comma = value_with_comma.replace('M','000000') \n",
    "\n",
    "    if '.0' in value_with_comma:\n",
    "        value_with_comma = value_with_comma.replace('.0', '')\n",
    "    if '.' in value_with_comma:\n",
    "        value_with_comma = value_with_comma.replace('.', '')\n",
    "        value_with_comma = value_with_comma.replace('0', '', 1)\n",
    "    \n",
    "    value_without_comma = value_with_comma\n",
    "    integer_value = int(value_without_comma)\n",
    "    print(f'integer_value:{integer_value}')\n",
    "    \n",
    "    return integer_value\n",
    "\n",
    "\n",
    "'''\n",
    "str_To_Number('1,059K')\n",
    "str_To_Number('1,000')\n",
    "str_To_Number('2.9K')\n",
    "str_To_Number('19K')\n",
    "str_To_Number('3,900K')\n",
    "str_To_Number('3.1M')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68046df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strList_To_NumberList(list_with_comma:list)->list:\n",
    "    print('** def strList_To_NumberList **')\n",
    "    print(f'value_with_comma:{list_with_comma}')\n",
    "    \n",
    "    list_without_comma = []\n",
    "    print('here')\n",
    "    for value_with_comma in list_with_comma:\n",
    "        if ',' in value_with_comma:\n",
    "            value_with_comma = value_with_comma.replace(',', '')\n",
    "        if 'K' in value_with_comma :\n",
    "            value_with_comma = value_with_comma.replace('K','000') \n",
    "        if 'M' in value_with_comma:\n",
    "                value_with_comma = value_with_comma.replace('M','000000') \n",
    "\n",
    "        if '.0' in value_with_comma:\n",
    "                value_with_comma = value_with_comma.replace('.0', '')\n",
    "        if '.' in value_with_comma:\n",
    "                value_with_comma = value_with_comma.replace('.', '')\n",
    "                value_with_comma = value_with_comma.replace('0', '', 1)\n",
    "        \n",
    "        value_without_comma = value_with_comma    \n",
    "        integer_value = int(value_without_comma)\n",
    "        list_without_comma.append(integer_value)\n",
    "    \n",
    "    \n",
    "    print(f'integer_value:{list_without_comma}')\n",
    "    \n",
    "    return list_without_comma\n",
    "\n",
    "\n",
    "#strList_To_NumberList(list_with_comma:list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ec97778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nClassify_influencerType('1,059K')\\nClassify_influencerType('1,000')\\nClassify_influencerType('2.9K')\\nClassify_influencerType('19K')\\nClassify_influencerType('3,900K')\\nClassify_influencerType('3.1M')\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Classify_influencerType(follower:str)-> str:\n",
    "    print('** def Classify_influencerType **')\n",
    "    \n",
    "    follower = str_To_Number(follower)\n",
    "    influencerType = ''\n",
    "    \n",
    "    if follower >= 1000000:\n",
    "        influencerType = 'MegaInfluencer'\n",
    "    elif follower < 1000000 and follower>=100000:\n",
    "        influencerType = 'MacroInfluencer'\n",
    "    elif follower < 100000 and follower>=1000:\n",
    "        influencerType = 'MicroInfluencer'\n",
    "    else:\n",
    "        influencerType = 'NanoInfluencer'\n",
    "    \n",
    "    \n",
    "    print(f'follower:{follower}, influencerType:{influencerType}')\n",
    "    \n",
    "    return influencerType \n",
    "    \n",
    "'''\n",
    "Classify_influencerType('1,059K')\n",
    "Classify_influencerType('1,000')\n",
    "Classify_influencerType('2.9K')\n",
    "Classify_influencerType('19K')\n",
    "Classify_influencerType('3,900K')\n",
    "Classify_influencerType('3.1M')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c44da1",
   "metadata": {},
   "source": [
    "## 3.3.1 Short Text Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbbcca",
   "metadata": {},
   "source": [
    "### 檢測是否包含hashtag、網址\n",
    "- 檢測此作者的文章是否有hashtag、網址，有的話標註包含次數在檔案中\n",
    "- 移除網址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d4aeae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# instance\\ntext = \"Noritake Marywood China, Salad Plates #2181, 8.25\" White Platinum Blue, Raised Floral, Kitchen Dining, MINT Condition, 12 Available https://tuppu.net/6205a45 #Etsy #AmazingFunVintage\"\\n# 呼叫函式偵測網址和hashtag\\nurls, hashtags = detect_urls_and_hashtags(text)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_urls_hashtags_metions(text:str)-> str:\n",
    "    #print('** detect_urls_and_hashtags **')\n",
    "\n",
    "    # 網址的正則表達式\n",
    "    url_pattern = r'https?://\\S+'\n",
    "    \n",
    "    # hashtag的正則表達式\n",
    "    hashtag_pattern = r'#\\w+'\n",
    "    \n",
    "    # 提及（@）的正則表達式\n",
    "    mention_pattern = r'@\\w+'\n",
    "    \n",
    "    urls = re.findall(url_pattern, text)\n",
    "    \n",
    "    hashtags = re.findall(hashtag_pattern, text)\n",
    "    \n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    \n",
    "    # 移除文字中的網址\n",
    "    text_removed = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    return text_removed, len(urls), len(hashtags), len(mentions) ##########計算提及##########\n",
    "\n",
    "'''\n",
    "# instance\n",
    "text = \"Noritake Marywood China, Salad Plates #2181, 8.25\\\" White Platinum Blue, Raised Floral, Kitchen Dining, MINT Condition, 12 Available https://tuppu.net/6205a45 #Etsy #AmazingFunVintage\"\n",
    "# 呼叫函式偵測網址和hashtag\n",
    "urls, hashtags = detect_urls_and_hashtags(text)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29b298",
   "metadata": {},
   "source": [
    "### 偵測並轉換emoji\n",
    "- 包含繪文字的話一樣計算出現次數，並將繪文字轉回情緒詞彙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e69a931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 範例文字包含表情符號\\ntext = \"I\\'m feeling 😀 and 😞\"\\ntext_with_names = detect_emoji(text)\\nprint(text_with_names)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_emoji(text:str)-> tuple:\n",
    "    #print('** detect_emoji **')\n",
    "    \n",
    "    #urls = re.findall(url_pattern, text)\n",
    "\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    emoji_count = sum(word.startswith(\":\") and word.endswith(\":\") for word in text)#############\n",
    "    \n",
    "    text_without_emoji = text.replace(\":\", \"\").replace(\"_face\", \"\")\n",
    "\n",
    "\n",
    "    # print(\"轉換後的文字:\", text_with_names)\n",
    "    return text_without_emoji, emoji_count\n",
    "\n",
    "'''\n",
    "# 範例文字包含表情符號\n",
    "text = \"I'm feeling 😀 and 😞\"\n",
    "text_with_names = detect_emoji(text)\n",
    "print(text_with_names)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f62b367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_post_stability(df):\n",
    "    \"\"\"\n",
    "    計算每個月的貼文數佔總貼文數的比率以及發文頻率的穩定度\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): 包含帖子日期的數據框（DataFrame）\n",
    "\n",
    "    Returns:\n",
    "    months (list): 每個月份的列表（字符串格式）\n",
    "    post_ratios (list): 每個月份貼文數量佔總貼文數的比率（浮點數格式）\n",
    "    post_stability (float): 發文頻率的穩定度（標準差）\n",
    "    total_posts (int): 總貼文數\n",
    "    \"\"\"\n",
    "    # 將帖子日期轉換為 datetime 格式\n",
    "    df['postDate'] = pd.to_datetime(df['postDate'])\n",
    "    \n",
    "    # 提取年份和月份，並添加新的欄位\n",
    "    df['year_month'] = df['postDate'].dt.to_period('M')\n",
    "    \n",
    "    # 計算每個月的帖子數量\n",
    "    monthly_counts = df['year_month'].value_counts().sort_index()\n",
    "    \n",
    "    # 計算總帖子數\n",
    "    total_posts = monthly_counts.sum()\n",
    "    \n",
    "    # 計算每個月份的帖子數量佔總帖子數的比率\n",
    "    post_ratios = monthly_counts / total_posts\n",
    "    \n",
    "    # 計算發文頻率的穩定度（標準差）\n",
    "    post_stability = np.std(post_ratios)\n",
    "    \n",
    "    # 將結果轉換為兩個 list\n",
    "    months = [str(month) for month in post_ratios.index]  # 將月份轉換為字符串格式\n",
    "\n",
    "    return months, post_ratios.tolist(), post_stability, total_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9594e8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## 推文總數\\ndef number_of_posts(df)->int:\\n    num_rows = df.shape[0]\\n    return num_rows\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## 推文總數\n",
    "def number_of_posts(df)->int:\n",
    "    num_rows = df.shape[0]\n",
    "    return num_rows\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f58f257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_ratios(textdata):\n",
    "    print('** def calculate_text_ratios **')\n",
    "\n",
    "    # 计算使用hashtag的比率\n",
    "    num_hashtags = textdata['numOfHashtags'].sum()\n",
    "    total_tweets = textdata.shape[0]\n",
    "    hashtag_ratio = num_hashtags / total_tweets\n",
    "\n",
    "    # 计算使用emoji的比率\n",
    "    num_emojis = textdata['numOfEmojis'].sum()\n",
    "    emoji_ratio = num_emojis / total_tweets\n",
    "\n",
    "    num_url = textdata['numOfUrls'].sum()\n",
    "    url_ratio = num_url / total_tweets\n",
    "    \n",
    "    num_mention = textdata['numOfMentions'].sum()\n",
    "    mention_ratio = num_url / total_tweets\n",
    "\n",
    "    return hashtag_ratio, emoji_ratio, url_ratio, mention_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776cb0d",
   "metadata": {},
   "source": [
    "## 3.3.2 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca771f1",
   "metadata": {},
   "source": [
    "## 清洗文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6eaba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map Penn Treebank POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(word:str):\n",
    "    #print('** get_wordnet_pos **')\n",
    "\n",
    "    # Get POS tag using nltk.pos_tag and map to WordNet POS\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wn.ADJ,  # Adjective\n",
    "                \"N\": wn.NOUN,  # Noun\n",
    "                \"V\": wn.VERB,  # Verb\n",
    "                \"R\": wn.ADV}   # Adverb\n",
    "    \n",
    "    return tag_dict.get(tag, wn.NOUN)  # Default to Noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e249a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(clean_text(\"\"I have 5 special invite codes available for  Join us and don\\'t forget to bring along your friends. Let\\'s embrace Zen! Oohm!\\\\ \" \" ))\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def text_preprocessing(text:str)->list:\n",
    "       \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  #刪去標點符號\n",
    "    text = text.lower() # 統一轉為小寫\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text) # 斷句 \n",
    "    tokens = [nltk.tokenize.word_tokenize(sent) for sent in sentences]  # 斷詞\n",
    "    \n",
    "    nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens[0] if token not in nltk_stopwords] # 僅保留非停用字(去除停用字)\n",
    "        \n",
    "    return tokens # 需要回傳list\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text:str)->str:\n",
    "    #print('** clean_text **')\n",
    "\n",
    "    # Remove non-English text\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    #text, url_count, hashtag_count = detect_urls_and_hashtags(text) # remove url and hashtags\n",
    "    #text = detect_emoji(text) # replace emoji to words\n",
    "    \n",
    "    # Remove emojis\n",
    "    #text = re.sub('[^\\w\\s,]', '', text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Lower Case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        pos = get_wordnet_pos(word)\n",
    "        if pos:\n",
    "            lemma = lemmatizer.lemmatize(word, pos)\n",
    "            lemmatized_words.append(lemma)\n",
    "    \n",
    "    # Join words back into text\n",
    "    cleaned_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "'''\n",
    "print(clean_text(\"\\\"I have 5 special invite codes available for  Join us and don't forget to bring along your friends. Let's embrace Zen! Oohm!\\ \\\" \" ))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e870a54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntweets = [\"Crypto is cool because it makes young people excited about the future, and that means something these days. Agree?\",\\n    \"her name is pretty.\",\\n \"dear @verizonsupport your service is straight 💩 in dallas.. been with y\\'all over a decade and this is all time low for y\\'all. i\\'m talking no internet at all.\",\\n \"@verizonsupport I sent you a dm\",\\n \"thanks to michelle et al at @verizonsupport who helped push my no-show-phone problem along. Order canceled successfully, and I ordered this for pickup today at the Apple store in the mall.\"\\n ]\\npolarity, subjectivity = Sentiment_Analysis(tweets)\\n\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 計算每一篇貼文的情緒 \n",
    "def Sentiment_Analysis(tweets:list):\n",
    "    print('** def Sentiment_Analysis **')\n",
    "    \n",
    "    polarity = []\n",
    "    subjectivity=[]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        blob = TextBlob(tweet)\n",
    "       # print(blob.sentiment) \n",
    "        polarity.append(blob.sentiment.polarity)  # polarity 的值在範圍 [-1, 1]，表示情感的正負程度\n",
    "        subjectivity.append(blob.sentiment.subjectivity)  #subjectivity 的值在範圍 [0, 1]，表示文本的主觀性。\n",
    "        print(f'Polarity: {blob.sentiment.polarity}, Subjectivity: {blob.sentiment.subjectivity}')\n",
    "        \n",
    "    return polarity, subjectivity\n",
    "    \n",
    "'''\n",
    "tweets = [\"Crypto is cool because it makes young people excited about the future, and that means something these days. Agree?\",\n",
    "    \"her name is pretty.\",\n",
    " \"dear @verizonsupport your service is straight 💩 in dallas.. been with y'all over a decade and this is all time low for y'all. i'm talking no internet at all.\",\n",
    " \"@verizonsupport I sent you a dm\",\n",
    " \"thanks to michelle et al at @verizonsupport who helped push my no-show-phone problem along. Order canceled successfully, and I ordered this for pickup today at the Apple store in the mall.\"\n",
    " ]\n",
    "polarity, subjectivity = Sentiment_Analysis(tweets)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa776c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfile_path = 'textData/elonmusk.csv'\\ndf = pd.read_csv(file_path)\\ncount_rate_of_pos_Sentiment(df)\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_rate_of_pos_Sentiment(df)->float:\n",
    "    print('** def count_rate_of_pos_Sentimentt **')\n",
    "    \n",
    "    numOfTweet = len(df)  # 統計總推文數量\n",
    "    PositiveTweet = df.loc[df['sentiment'] > 0]  \n",
    "    numOfPositive = len(PositiveTweet)  # 統計正向推文數量\n",
    "    rate_of_post_Sentiment = numOfPositive / numOfTweet  # 計算正向推文佔比\n",
    "    \n",
    "    print(f'正貼文比: {rate_of_post_Sentiment}')\n",
    "    return rate_of_post_Sentiment\n",
    "\n",
    "\n",
    "'''\n",
    "file_path = 'textData/elonmusk.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "count_rate_of_pos_Sentiment(df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c0661d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_avg_Subjectivity(textdata):\n",
    "    \n",
    "    return textdata['subjectivity'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a083b8",
   "metadata": {},
   "source": [
    "## 3.3.3\tTopic Analysis\n",
    "GENSIM document: https://radimrehurek.com/gensim/models/ldamulticore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5dd5a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndocuments = [\\n    \"topic modeling is interesting and useful\",\\n    \"gensim library provides tools for topic modeling\",\\n    \"LDA is a popular algorithm for topic modeling\",\\n    \"topic modeling helps in discovering hidden patterns in text data\",\\n    \"NLP techniques are often used in topic modeling\"\\n]\\n\\nnum_topics = 2\\nlda_model = train_LDA_model(documents, num_topics)\\n\\nprint(\"LDA Topics:\")\\nprint(lda_model.print_topics())\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_LDA_model(documents:list, num_topics:int):\n",
    "    print('** def train_LDA_model **')\n",
    "\n",
    "    # 斷詞\n",
    "    texts = [[word for word in document.split()] for document in documents]\n",
    "\n",
    "    # 建詞典\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # 建詞频矩陣\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # train LDA model\n",
    "    #lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=20) #這個速度比較慢\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics)\n",
    "\n",
    "    #print(lda_model.print_topics())\n",
    "    return lda_model\n",
    "\n",
    "'''\n",
    "documents = [\n",
    "    \"topic modeling is interesting and useful\",\n",
    "    \"gensim library provides tools for topic modeling\",\n",
    "    \"LDA is a popular algorithm for topic modeling\",\n",
    "    \"topic modeling helps in discovering hidden patterns in text data\",\n",
    "    \"NLP techniques are often used in topic modeling\"\n",
    "]\n",
    "\n",
    "num_topics = 2\n",
    "lda_model = train_LDA_model(documents, num_topics)\n",
    "\n",
    "print(\"LDA Topics:\")\n",
    "print(lda_model.print_topics())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46cc3d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfile_path = 'Airdrops0637.csv'\\ndf = pd.read_csv(file_path)\\ncreator_text = df['CleanedText'].tolist()\\ndocument_topics = topic_analysis(documents=creator_text, lda_model=lda_model)\\n#print(document_topics)\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topic_analysis(documents:list, lda_model):\n",
    "   # print('** def topic_analysis **')\n",
    "    document_topics = []\n",
    "    for document in documents:\n",
    "        # 分词处理\n",
    "        words = [word for word in document.split()]\n",
    "        # 将文档转换为词袋表示\n",
    "        document_bow = lda_model.id2word.doc2bow(words)\n",
    "        # 获取文档的主题分布\n",
    "        topics = lda_model.get_document_topics(document_bow)\n",
    "        # 选择具有最高概率的主题作为分类标签\n",
    "        if topics:\n",
    "            dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            dominant_topic = None\n",
    "        document_topics.append(dominant_topic)\n",
    "    return document_topics\n",
    "\n",
    "\n",
    "'''\n",
    "file_path = 'Airdrops0637.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "creator_text = df['CleanedText'].tolist()\n",
    "document_topics = topic_analysis(documents=creator_text, lda_model=lda_model)\n",
    "#print(document_topics)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77244b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntopic_tag = [2, 8, 4, 2, 6, 4, 9, 0, 1, 5, 0, 7, 6, 0, 5, 3, 6, 7, 1, 6, 3, 0, 9, 7, 4, 1, 2, 1, 0, 0, 4, 2, 3, 6, 1, 3, 2, 8, 2, 6, 5, 0, 8, 4, 5, 6, 1, 1, 9, 9, 5, 4, 4, 1, 1, 8, 4, 1, 6, 7, 5, 6, 8, 5, 1, 0, 8, 6, 5, 1, 2, 0, 4, 8, 3, 2, 9, 4, 6, 6, 8, 1, 6, 4, 9, 2, 4, 6, 7, 0, 6, 4, 5, 8, 7, 6, 0, 9, 5, 8, 6, 1, 8, 0, 8, 1, 5, 1, 1, 3, 4, 3, 4, 3, 3, 6, 0, 3, 4, 3, 4, 3, 5, 8, 5, 8, 3, 8, 6, 2, 2, 1, 9, 1, 6, 5, 8, 0, 2, 8, 4, 5, 8, 6, 1, 4, 8, 5, 5, 8, 0, 6, 5, 1]\\nposts_per_topic = count_posts_per_topic(topic_tag=topic_tag, num_of_topics=10)\\n\\nprint(posts_per_topic)\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def count_posts_per_topic(topic_tag: list, num_of_topics: int):\n",
    "    print('** def count_posts_per_topic **')\n",
    "    \n",
    "    posts_per_topic = {}\n",
    "    for topic_index in range(num_of_topics):  \n",
    "        count = topic_tag.count(topic_index)\n",
    "        topic_name = f'topic{topic_index+1}'# 主題索引從1開始\n",
    "        posts_per_topic[topic_name] = count/len(topic_tag)\n",
    "    df = pd.DataFrame.from_dict(posts_per_topic, orient='index', columns=['Count'])\n",
    "    \n",
    "    return df #posts_per_topic \n",
    "\n",
    "'''\n",
    "\n",
    "topic_tag = [2, 8, 4, 2, 6, 4, 9, 0, 1, 5, 0, 7, 6, 0, 5, 3, 6, 7, 1, 6, 3, 0, 9, 7, 4, 1, 2, 1, 0, 0, 4, 2, 3, 6, 1, 3, 2, 8, 2, 6, 5, 0, 8, 4, 5, 6, 1, 1, 9, 9, 5, 4, 4, 1, 1, 8, 4, 1, 6, 7, 5, 6, 8, 5, 1, 0, 8, 6, 5, 1, 2, 0, 4, 8, 3, 2, 9, 4, 6, 6, 8, 1, 6, 4, 9, 2, 4, 6, 7, 0, 6, 4, 5, 8, 7, 6, 0, 9, 5, 8, 6, 1, 8, 0, 8, 1, 5, 1, 1, 3, 4, 3, 4, 3, 3, 6, 0, 3, 4, 3, 4, 3, 5, 8, 5, 8, 3, 8, 6, 2, 2, 1, 9, 1, 6, 5, 8, 0, 2, 8, 4, 5, 8, 6, 1, 4, 8, 5, 5, 8, 0, 6, 5, 1]\n",
    "posts_per_topic = count_posts_per_topic(topic_tag=topic_tag, num_of_topics=10)\n",
    "\n",
    "print(posts_per_topic)\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e6dbd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ndef append_num_of_topic_to_csv(posts_per_topic: dict, csv_filename: str):\\n    print('** def append_num_of_topic_to_csv **')\\n    \\n    # 將字典轉換為 DataFrame\\n    df = pd.DataFrame.from_dict(posts_per_topic, orient='index', columns=['Value'])\\n    \\n    # 將 DataFrame 寫入 CSV 文件，並指定 index_label 為索引列\\n    df.to_csv(csv_filename, mode='a', header=False, index_label='Index')\\n   \\n    return\\n   \\nappend_num_of_topic_to_csv(posts_per_topic=posts_per_topic, csv_filename='text.csv')\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "def append_num_of_topic_to_csv(posts_per_topic: dict, csv_filename: str):\n",
    "    print('** def append_num_of_topic_to_csv **')\n",
    "    \n",
    "    # 將字典轉換為 DataFrame\n",
    "    df = pd.DataFrame.from_dict(posts_per_topic, orient='index', columns=['Value'])\n",
    "    \n",
    "    # 將 DataFrame 寫入 CSV 文件，並指定 index_label 為索引列\n",
    "    df.to_csv(csv_filename, mode='a', header=False, index_label='Index')\n",
    "   \n",
    "    return\n",
    "   \n",
    "append_num_of_topic_to_csv(posts_per_topic=posts_per_topic, csv_filename='text.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f03ff81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 使用範例\\ntopic_folder = \"dataset/allTextTopic\"\\nuser_folder = \"dataset/userdata\"\\nget_topic_and_write(topic_folder, user_folder)\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_topic_and_write(topic_folder: str, user_folder: str):\n",
    "    # 檢查資料夾是否存在\n",
    "    if not os.path.exists(topic_folder) or not os.path.exists(user_folder):\n",
    "        print(\"指定的資料夾不存在。\")\n",
    "        return\n",
    "    \n",
    "    # 遍歷主題資料夾中的所有文件\n",
    "    for topic_file in os.listdir(topic_folder):\n",
    "        \n",
    "        # 讀取主題 CSV 文件\n",
    "        try:\n",
    "            topic_path = os.path.join(topic_folder, topic_file)\n",
    "            topic_df = pd.read_csv(topic_path)\n",
    "        except Exception as e:\n",
    "            print(\"無法讀取主題文件:\", e)\n",
    "            continue\n",
    "        try:\n",
    "            user_path = os.path.join(user_folder, topic_file)\n",
    "            user_path = user_path.replace('.csv','_bg.csv')\n",
    "            user_df = pd.read_csv(user_path)\n",
    "            \n",
    "            \n",
    "            # 找到主題\n",
    "            topic_count = topic_df['Count']\n",
    "            idx = np.where(topic_count == 1)[0]\n",
    "            topic = topic_count.index[idx[0]]\n",
    "        \n",
    "            print(f'-----{topic}-----')\n",
    "            # 將主題資訊應用到使用者 DataFrame 中\n",
    "            user_df['creator_topic'] = topic\n",
    "            \n",
    "            # 保存修改後的使用者 DataFrame\n",
    "            user_df.to_csv(user_path, index=False)\n",
    "            print(f\"已成功將主題資訊應用到使用者文件 {user_path} 中並保存。\")\n",
    "        except Exception as e:\n",
    "            print(f\"無法保存使用者文件 {user_path}:\", e)\n",
    "'''\n",
    "# 使用範例\n",
    "topic_folder = \"dataset/allTextTopic\"\n",
    "user_folder = \"dataset/userdata\"\n",
    "get_topic_and_write(topic_folder, user_folder)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6bced5",
   "metadata": {},
   "source": [
    "## 3.4.1\tPersonality Trait Analysis\n",
    "By 東穎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4d92a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Penn Treebank POS tag to WordNet POS tag\n",
    "def ptb_to_wn(tag):  \n",
    "    #print('** def ptb_to_wn **')\n",
    "\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    return None  # Return None for other cases\n",
    "\n",
    "# Function to convert tagged word to WordNet synset\n",
    "def tagged_to_synset(word, tag):\n",
    "    #print('** def tagged_to_synset **')\n",
    "    \n",
    "    wn_tag = ptb_to_wn(tag)  # Convert Penn Treebank POS tag to WordNet POS tag\n",
    "    if wn_tag is None:\n",
    "        return None  # Return None if POS tag is not recognized \n",
    "    try:\n",
    "        # Get the first synset for the word and POS tag\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None  # Return None if no synsets are found\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf3e206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the similarity score between two sentences\n",
    "def sentence_similarity(s1, s2):    \n",
    "    #print('** def sentence_similarity **')\n",
    "    \n",
    "    # Tokenize and POS tag the input sentences\n",
    "    s1 = pos_tag(word_tokenize(s1))\n",
    "    s2 = pos_tag(word_tokenize(s2)) \n",
    "\n",
    "    # Convert POS-tagged words to WordNet synsets\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in s1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in s2]\n",
    "\n",
    "    # Remove \"None\" values from synsets\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "\n",
    "    score, count = 0.0, 0\n",
    "\n",
    "    # Calculate the similarity score for each synset in the first sentence\n",
    "    for synset in synsets1:\n",
    "        try:\n",
    "            # Find the best similarity score with synsets in the second sentence\n",
    "            best_score = max([synset.path_similarity(ss) for ss in synsets2])\n",
    "            \n",
    "            if best_score is not None:\n",
    "                score += best_score\n",
    "                count += 1\n",
    "        except:\n",
    "            score = 0  # Handle exceptions by setting score to 0\n",
    "\n",
    "    try:\n",
    "        score /= count  # Calculate the average score\n",
    "    except ZeroDivisionError:\n",
    "        score = 0  # Set score to 0 if there are no valid scores\n",
    "\n",
    "    return score\n",
    "\n",
    "# Function to compute symmetric sentence similarity\n",
    "def symSentSim(s1, s2):\n",
    "    #print('** def symSentSim **')\n",
    "    # Calculate the symmetric sentence similarity score\n",
    "    sss_score = (sentence_similarity(s1, s2) + sentence_similarity(s2, s1)) / 2\n",
    "    return sss_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5bc53b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\ndf = pd.read_csv('chelice_bts.csv')\\npersonality_analysis(df['textresponding'], 'chelice_bts.csv')\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extraversion = ['active', 'adventurous', 'affectionate', 'assertive', 'boisterous', 'bold', 'cheerful', 'confident', 'dominant', 'energetic', 'enthusiastic', 'extroverted', 'friendly', 'gregarious', 'impulsive', 'loud', 'outgoing', 'playful', 'self-assured', 'sociable', 'spontaneous', 'talkative', 'thrill-seeking', 'vivacious', 'warm', 'fun-loving', 'risk-taking', 'party-loving', 'stimulation-seeking', 'high-spirited', 'life-of-the-party', 'people-person', 'social-butterfly', 'charismatic', 'daring', 'gregariousness', 'assertiveness', 'excitement-seeking', 'positive-affect', 'activity', 'optimism', 'happiness', 'affiliation', 'extraversion', 'outgoingness', 'sensation-seeking', 'cheerfulness', 'friendliness']\n",
    "# agreeableness = ['affectionate', 'appreciative', 'caring', 'compassionate', 'considerate', 'cooperative', 'empathetic', 'forgiving', 'generous', 'helpful', 'kind', 'likable', 'nurturing', 'pleasant', 'polite', 'sensitive', 'sweet', 'sympathetic', 'thoughtful', 'tolerant', 'trusting', 'warm', 'altruistic', 'charitable', 'friendly', 'unselfish', 'gentle', 'tender', 'good-natured', 'cooperative', 'agreeableness', 'empathy', 'love', 'teamwork', 'modesty', 'generosity', 'consideration', 'kindness', 'pleasantness', 'gentleness', 'cooperation', 'affiliation', 'interpersonal-sensitivity', 'warmth', 'helpfulness', 'trust', 'compassion', 'friendliness']\n",
    "# conscientiousness = ['competent', 'confident', 'determined', 'disciplined', 'dutiful', 'efficient', 'focused', 'goal-oriented', 'hard-working', 'organized', 'punctual', 'reliable', 'responsible', 'self-disciplined', 'thorough', 'thoughtful', 'ambitious', 'self-controlled', 'dependable', 'industrious', 'productive', 'persistent', 'practical', 'structured', 'systematic', 'orderly', 'methodical', 'achievement-striving', 'cautiousness', 'dutifulness', 'orderliness', 'achievement', 'responsibility', 'deliberation', 'dependability', 'organization', 'perseverance', 'self-efficacy', 'self-discipline', 'conscientiousness', 'work-ethic', 'planning', 'perseverance', 'industriousness', 'accuracy', 'detail-orientation', 'reliability', 'goal-directedness']\n",
    "# neuroticism = ['anxious', 'apprehensive', 'depressed', 'emotional', 'fearful', 'frustrated', 'gloomy', 'guilty', 'harassed', 'hopeless', 'hysterical', 'insecure', 'irritable', 'jittery', 'lonesome', 'melancholy', 'nervous', 'panicky', 'paralyzed', 'paranoid', 'perturbed', 'pessimistic', 'sad', 'scared', 'shaky', 'startled', 'suspicious', 'tense', 'terrified', 'threatened', 'timid', 'troubled', 'unhappy', 'vulnerable', 'weepy', 'worried', 'fearfulness', 'anxiety', 'anger', 'depression', 'self-consciousness', 'immoderation', 'vulnerability', 'stress', 'self-doubt', 'rumination', 'negativity', 'moodiness', 'fear', 'hostility', 'shame', 'insecurity', 'hypersensitivity', 'emotional-reactivity', 'worry', 'hopelessness', 'neuroticism']\n",
    "# openness = ['artistic', 'creative', 'curious', 'imaginative', 'inventive', 'original', 'unconventional', 'wide interests', 'open-minded', 'broad-minded', 'innovative', 'intelligent', 'knowledgeable', 'perceptive', 'philosophical', 'reflective', 'sophisticated', 'unconventional', 'visionary', 'intellectual', 'wisdom', 'aesthetic-appreciation', 'emotional-awareness', 'fantasy', 'creativity', 'openness-to-experience', 'divergent-thinking', 'interests', 'imagination', 'abstract-thinking', 'complexity', 'originality', 'intellect', 'unconventionality', 'appreciation-for-art', 'insightfulness', 'liberalism', 'openness']\n",
    "def personality_analysis(df, file_path:str):\n",
    "    print('** def personality_analysis **')\n",
    "    \n",
    "    try:\n",
    "        extraversion = ['talkative', 'assertive', 'enthusiasm', 'energetic', 'adventure', 'dominance', 'social', 'excitement', 'fun', 'optimism']\n",
    "        agreeableness = ['politeness', 'helpful', 'kind', 'empathy', 'cooperation', 'modesty', 'affection', 'sympathy', 'pleasant', 'trust']\n",
    "        conscientiousness = ['achievement', 'striving', 'planning', 'organized', 'dutiful', 'discipline', 'work', 'responsible', 'dependable', 'perseverance']\n",
    "        neuroticism = ['anxiety', 'anger', 'depression', 'emotional', 'stress', 'vulnerability', 'fear', 'nervous', 'tense', 'worry']\n",
    "        openness = ['insight', 'curious', 'interest', 'imagination', 'unconventional', 'originality', 'creativity', 'art', 'novel', 'idea']\n",
    "        o = ' '.join(openness)\n",
    "        c = ' '.join(conscientiousness)\n",
    "        e = ' '.join(extraversion)\n",
    "        a = ' '.join(agreeableness)\n",
    "        n = ' '.join(neuroticism)\n",
    "\n",
    "\n",
    "    \n",
    "        #df['CleanedText'] = df['textWithoutEmoji'].apply(clean_text)\n",
    "\n",
    "        # 比對詞彙意義相似度\n",
    "        df['O_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, o)) \n",
    "        df['C_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, c))\n",
    "        df['E_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, e))\n",
    "        df['A_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, a))\n",
    "        df['N_Score'] = df['CleanedText'].apply(lambda x: symSentSim(x, n))\n",
    "        print('Get OCEAN Score success!')\n",
    "    except Exception as e:\n",
    "        print(\"計算大五人格分數時出現錯誤:\", e)\n",
    "    \n",
    "    try:\n",
    "        big5_file_path = file_path.replace('.csv','_big5.csv')#save(new file)\n",
    "        df.to_csv(big5_file_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"無法正確保存大五人格分數:\", e)\n",
    "\n",
    "'''    \n",
    "df = pd.read_csv('chelice_bts.csv')\n",
    "personality_analysis(df['textresponding'], 'chelice_bts.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba458d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 调用函数并输出 OCEAN 各自的平均分数\\nuser_file_path = 'seokjinism1_bg.csv'\\ntext_file_path = 'seokjinism1.csv'  # 请替换为你的文件路径\\ncalculate_ocean_avg_scores(user_file_path,text_file_path)\\n\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_ocean_avg_scores(user_file_path: str, text_file_path:str):\n",
    "    print('** def calculate_ocean_avg_scores **')\n",
    "\n",
    "    try:\n",
    "        # 从文件中读取数据\n",
    "        userFile = pd.read_csv(user_file_path)\n",
    "        textFile = pd.read_csv(text_file_path)\n",
    "        \n",
    "        # 计算每个维度的平均分数\n",
    "        userFile['O_Score'] = [textFile['O_Score'].mean()]\n",
    "        userFile['C_Score'] = [textFile['C_Score'].mean()]\n",
    "        userFile['E_Score'] = [textFile['E_Score'].mean()]\n",
    "        userFile['A_Score'] = [textFile['A_Score'].mean()]\n",
    "        userFile['N_Score'] = [textFile['N_Score'].mean()]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"计算 OCEAN 平均分数时出现错误:\", e)\n",
    "\n",
    "    try:\n",
    "        userFile.to_csv(user_file_path, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"存檔错误:\", e)\n",
    "\n",
    "'''\n",
    "# 调用函数并输出 OCEAN 各自的平均分数\n",
    "user_file_path = 'seokjinism1_bg.csv'\n",
    "text_file_path = 'seokjinism1.csv'  # 请替换为你的文件路径\n",
    "calculate_ocean_avg_scores(user_file_path,text_file_path)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "800fcdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 调用函数并输出 OCEAN 各自的平均分数\\nuser_file_path = 'seokjinism1_bg.csv'\\ntext_file_path = 'seokjinism1.csv'  # 请替换为你的文件路径\\ncalculate_ocean_avg_scores(user_file_path,text_file_path)\\n\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_follower_ocean_avg_scores(user_file_path: str, text_file_path:str):\n",
    "    print('** def calculate_ocean_avg_scores **')\n",
    "\n",
    "    try:\n",
    "        # 从文件中读取数据\n",
    "        userFile = pd.read_csv(user_file_path)\n",
    "        textFile = pd.read_csv(text_file_path)\n",
    "        \n",
    "        # 计算每个维度的平均分数\n",
    "        userFile['Follower_O_Score'] = [textFile['O_Score'].mean()]\n",
    "        userFile['Follower_C_Score'] = [textFile['C_Score'].mean()]\n",
    "        userFile['Follower_E_Score'] = [textFile['E_Score'].mean()]\n",
    "        userFile['Follower_A_Score'] = [textFile['A_Score'].mean()]\n",
    "        userFile['Follower_N_Score'] = [textFile['N_Score'].mean()]\n",
    "        print(textFile['N_Score'].mean())\n",
    "    except Exception as e:\n",
    "        print(\"计算 OCEAN 平均分数时出现错误:\", e)\n",
    "\n",
    "    try:\n",
    "        userFile.to_csv(user_file_path, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"存檔错误:\", e)\n",
    "\n",
    "'''\n",
    "# 调用函数并输出 OCEAN 各自的平均分数\n",
    "user_file_path = 'seokjinism1_bg.csv'\n",
    "text_file_path = 'seokjinism1.csv'  # 请替换为你的文件路径\n",
    "calculate_ocean_avg_scores(user_file_path,text_file_path)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071428f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0b5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
